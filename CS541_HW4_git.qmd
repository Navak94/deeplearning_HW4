---
title: "CS/DS 541: Deep Learning"
subtitle: "Homework 4"
author: "Tom Arnold & Nate Hindman"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
         
         \usepackage{multicol}
         \newcommand{\btwocol}{\begin{multicols}{2}}
         \newcommand{\etwocol}{\end{multicols}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
      % Note: setting commandchars=\\\{\} here will cause an error 
    }
---

\btwocol

*Due: 5:59pm ET Thursday October 9*

*This problem can be done in teams of up 2 students.*

\etwocol


# 1 Window Type Classification [20 points]

In this problem, the goal is to classify window images into one of five categories: "New Awning Window", "New Bay Window", "New Fixed Window", "New Horizontal Sliding Window", and "New Hung Window".

The training and test datasets can be accessed via the following links:
* [https://canvas.wpi.edu/files/7719816/download?download_frd=1](https://canvas.wpi.edu/files/7719816/download?download_frd=1) and
* [https://canvas.wpi.edu/files/7719811/download?download_frd=1](https://canvas.wpi.edu/files/7719811/download?download_frd=1).

To help you get started, a demo code are available at:
[https://colab.research.google.com/drive/1fG0f6LiPnv7a4nDWNPtVyo1Y0Xh5vp71?usp=sharing](https://colab.research.google.com/drive/1fG0f6LiPnv7a4nDWNPtVyo1Y0Xh5vp71?usp=sharing).

---

## Answer

```python

# import packages
import pandas as pd
import numpy as np
import torch
import json
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


# connect to google drive
from google.colab import drive
drive.mount('/content/drive/')

# Step 1

# NEW TOM + NATE BLOCK
import torch.nn as nn

class betterCNN(nn.Module):
    def __init__(self, num_classes=5):
        super(betterCNN, self).__init__()

        # convolutional block
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)
        self.bn1 = nn.BatchNorm2d(64)
        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)
        self.bn2 = nn.BatchNorm2d(64)

        # convolutional block
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)
        self.bn3 = nn.BatchNorm2d(128)
        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)
        self.bn4 = nn.BatchNorm2d(128)

        # convolutional block
        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)
        self.bn5 = nn.BatchNorm2d(256)
        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)
        self.bn6 = nn.BatchNorm2d(256)

        # pooling and dropout
        self.pool = nn.MaxPool2d(2, 2)
        self.dropout = nn.Dropout(0.2)

        # fully connected layers
        self.fc1 = nn.Linear(256 * 8 * 8, 512)
        self.fc2 = nn.Linear(512, num_classes)

    def forward(self, x):
        # 1 block
        x = self.conv1(x)
        x = self.bn1(x)
        x = torch.relu(x)
        x = self.conv2(x)
        x = self.bn2(x)
        x = torch.relu(x)
        x = self.pool(x)

        # 2 block
        x = self.conv3(x)
        x = self.bn3(x)
        x = torch.relu(x)
        x = self.conv4(x)
        x = self.bn4(x)
        x = torch.relu(x)
        x = self.pool(x)

        # 3 block
        x = self.conv5(x)
        x = self.bn5(x)
        x = torch.relu(x)
        x = self.conv6(x)
        x = self.bn6(x)
        x = torch.relu(x)
        x = self.pool(x)

        # flatten
        x = x.view(-1, 256 * 8 * 8)

        # fully connected layers
        x = self.fc1(x)
        x = torch.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)

        return x

model = betterCNN(num_classes=5)
model = model.to(device)
print(model)

# Step 2

# NEW TOM + NATE BLOCK

from torchvision import datasets, transforms
from torch.utils.data import DataLoader, random_split

# transform with augmentation
train_transform = transforms.Compose([
    transforms.Resize((64, 64)),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomRotation(degrees=15),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

# val transform (no augmentation)
val_transform = transforms.Compose([
    transforms.Resize((64, 64)),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

trainset_dir = '/content/drive/MyDrive/train_set'

full_dataset = datasets.ImageFolder(root=trainset_dir, transform=None)

total_size = len(full_dataset)
indices = list(range(total_size))
np.random.seed(42)
np.random.shuffle(indices)

train_size = int(0.85 * total_size)
val_size = total_size - train_size

train_indices = indices[0:train_size]
val_indices = indices[train_size:total_size]

train_dataset = datasets.ImageFolder(root=trainset_dir, transform=train_transform)
train_subset = torch.utils.data.Subset(train_dataset, train_indices)

val_dataset = datasets.ImageFolder(root=trainset_dir, transform=val_transform)
val_subset = torch.utils.data.Subset(val_dataset, val_indices)

train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)
val_loader = DataLoader(val_subset, batch_size=64, shuffle=False)


# Step 3

# NEW TOM + NATE BLOCK

import torch.optim as optim
criterion = nn.CrossEntropyLoss(label_smoothing=0.09) # added label smoothing
optimizer = optim.Adam(model.parameters(), lr=0.002, weight_decay=0.00001) # added weight decay
# added learning rate scheduler
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer,
    mode='max',
    factor=0.5,
    patience=5)

# Step 4

# NEW TOM + NATE BLOCK

def train(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=30):
    best_val_acc = 0.0
    patience_counter = 0
    early_stop_patience = 10

    train_losses = []
    val_accuracies = []

    for epoch in range(num_epochs):
        # train
        model.train()
        running_loss = 0.0
        train_correct = 0
        train_total = 0

        for inputs, labels in train_loader:
            inputs = inputs.to(device)
            labels = labels.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss = running_loss + loss.item()

            predictions = torch.max(outputs, 1)[1]
            train_total = train_total + labels.size(0)
            train_correct = train_correct + (predictions == labels).sum().item()

        avg_train_loss = running_loss / len(train_loader)
        train_acc = 100.0 * train_correct / train_total
        train_losses.append(avg_train_loss)

        # val
        model.eval()
        val_correct = 0
        val_total = 0

        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs = inputs.to(device)
                labels = labels.to(device)
                outputs = model(inputs)
                predictions = torch.max(outputs, 1)[1]
                val_total = val_total + labels.size(0)
                val_correct = val_correct + (predictions == labels).sum().item()

        val_acc = 100.0 * val_correct / val_total
        val_accuracies.append(val_acc)

        print(f"Epoch {epoch + 1}/{num_epochs}")
        print(f" -> Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%")
        print(f" -> Val Acc: {val_acc:.2f}%")

        # new scheduler step
        scheduler.step(val_acc)

        # save state added
        if val_acc >= best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), 'best_model.pth')
            print(f"----> Best model saved ---> Val Acc: {val_acc:.2f}%")
            patience_counter = 0
        else:
            patience_counter = patience_counter + 1

        # early stopping
        if patience_counter >= early_stop_patience:
            break

        print()

    return train_losses, val_accuracies

train_losses, val_accuracies = train(
    model,
    train_loader,
    val_loader,
    criterion,
    optimizer,
    scheduler,
    num_epochs=100
)


# Step 5

# NEW TOM + NATE BLOCK

def evaluate(model, val_loader):
    model.eval()
    correct = 0
    total = 0

    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs = inputs.to(device)
            labels = labels.to(device)
            outputs = model(inputs)
            predictions = torch.max(outputs, 1)[1]
            total = total + labels.size(0)
            correct = correct + (predictions == labels).sum().item()

    accuracy = 100.0 * correct / total
    print(f'Val Accuracy: {accuracy:.2f}%')
    return accuracy

model.load_state_dict(torch.load('best_model.pth'))
evaluate(model, val_loader)

Please don't make any change after this line. The only parameters you may modify are those within the "test_transform" function.

import os
from PIL import Image
class CustomImageDataset(torch.utils.data.Dataset):
    def __init__(self, folder_path, transform=None):
        self.folder_path = folder_path
        self.transform = transform
        self.image_paths = [os.path.join(folder_path, filename) for filename in os.listdir(folder_path) if filename.endswith(('png', 'jpg', 'jpeg'))]

    def __len__(self):
        return len(self.image_paths)

    def filename2index(self, filename):
        return os.path.basename(filename).replace('.jpg', '')

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        img = Image.open(img_path).convert('RGB')
        if self.transform:
            img = self.transform(img)
        return img, self.filename2index(img_path)

test_folder = '/content/drive/MyDrive/test_set'
test_transform = transforms.Compose([
    transforms.Resize((64, 64)),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])
test_dataset = CustomImageDataset(test_folder, transform=test_transform)
test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)

class_to_idx = train_dataset.class_to_idx
idx_to_class = {v: k for k, v in class_to_idx.items()}

# Make predictions
def evaluate_model(model, test_loader, idx_to_class):
    all_predictions = {}
    with torch.no_grad():
        for inputs, index in test_loader:
            inputs = inputs.to(device)

            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)
            predicted_class = predicted.item()
            predicted_class_name = idx_to_class[predicted_class]
            all_predictions[index[0]] = predicted_class_name

    return all_predictions

predictions = evaluate_model(model, test_loader, idx_to_class)
with open('predictions.json', 'w') as json_file:
    json.dump(predictions, json_file, indent=4)

print("Evaluation completed and predictions saved.")

# you may need to install thop when you first run this code
!pip install thop

# Compute FLOPs using thop
import thop
input_tensor = test_dataset[0][0].unsqueeze(0).to(device) # must have exact same size of the data input (batch, channel, height, width) and be on the same device as the model
flops, params = thop.profile(model, inputs=(input_tensor,))
print(f"FLOPs: {flops}")
print(f"Number of Parameters: {params}")
flops_and_params = {
    "FLOPs": flops,
    "Parameters": params
}

output_json_path = 'flops_and_params.json'

with open(output_json_path, 'w') as json_file:
    json.dump(flops_and_params, json_file, indent=4)

print(f"FLOPs and parameters have been saved to {output_json_path}")

```


{{< embed cnn_improved.ipynb#prob1 >}}

---

# 2 Comparing Vanilla RNN with Variants in Sequence Modeling [20 points]

You will implement and train three different neural networks for sequence modeling: a Vanilla RNN (a simple RNN with shared weights), and two variants of a NN with a similar architecture to the Vanilla RNN but which do not share weights.

You will compare their performance on a sequence prediction task and analyze the differences between them.

Prediction task: this is a many-to-one regression task, i.e., a sequence of inputs is used for predicting a single output.
In particular, the i-th input sequence $\mathcal{X}^{(i)}=(x_{1}^{(i)},...,x_{l_{i}}^{(i)})$ has length $l_{i}$, where $x_{j}^{(i)}\in\mathbb{R}^{10}$, the i-th output is a scalar $y^{(i)}\in\mathbb{R}$.

Dataset: You will use a synthetic dataset containing sequences of variable lengths stored in the zip file homework5_question2_data.zip. Each sequence consists of input features and corresponding target values. The sequences are generated such that they represent a time-dependent process. Note that $l_{i}$ may be different than $l_{j}$ for $i\ne j$. So the (pickled) numpy object X is actually a list of sequences.

Tasks:

1. (4 points) Implement a Vanilla RNN: Implement a Vanilla RNN architecture (needless to say, weights are shared across time steps). A pytorch starter code is provided in homework4_starter.py. Important: You are not allowed to use an RNN layer implementation from any library.

2. (4 points) Implement a NN with Sequences Truncated to the Same Length: Implement a NN where sequences are truncated to have the same length before training. In other words, if the shortest sequence in the dataset has length L, all sequences should be truncated to length L before training.

3. (4 points) Implement a NN with Sequences Padded to the Same Length: Implement another variant of NN where sequences are padded to have the same length before training. Use appropriate padding techniques to ensure that all sequences have the same length, and implement a mechanism to ignore the padding when computing loss and predictions.

4. Train and Compare the Models:
    (a) (1 point) Train all three models (Vanilla RNN, Truncated NN, Padded NN) on the provided dataset.
    (b) (1 point) Use a suitable loss function for sequence prediction tasks, such as mean squared error (MSE) or cross-entropy.
    (c) (1 point) Train each model for a fixed number of epochs or until convergence.
    (d) (1 point) Monitor and record performance metrics, such as training loss, on a validation set during training.

5. Evaluate and Compare the Models:
    (a) (1 point) Evaluate the trained models on a separate test dataset.
    (b) (2 point) Compare the performance of the three models in terms of MSE, convergence speed, and overfitting tendencies.
    (c) (1 point) Analyze the results and discuss the advantages and disadvantages of each approach in terms of modeling sequences with varying lengths.

Additional Information:
You can choose the specific hyperparameters for your models, such as the number of hidden units, learning rate, batch size, and sequence length. Feel free to use any deep learning framework or library you are comfortable with, and provide clear code documentation. Note: Be sure to clearly explain your implementation, provide code comments, and present your results in a well-organized manner in the report.

---

## Answer

```python
# connect to google drive
from google.colab import drive
drive.mount('/content/drive/')

path = '/content/drive/MyDrive/'

import torch
import torch.nn as nn
import torch.optim as optim
from torch.nn.utils.rnn import pad_sequence
import numpy as np
import random
import os
os.chdir(path)


# load training and test data
def loadData():
    X_train = np.load('X_train.npy',allow_pickle=True)
    y_train = np.load('y_train.npy',allow_pickle=True)
    X_test = np.load('X_test.npy',allow_pickle=True)
    y_test = np.load('y_test.npy',allow_pickle=True)

    X_train = [torch.Tensor(x) for x in X_train]  # List of Tensors (SEQ_LEN[i],INPUT_DIM) i=0..NUM_SAMPLES-1
    X_test = [torch.Tensor(x) for x in X_test]  # List of Tensors (SEQ_LEN[i],INPUT_DIM)
    y_train = torch.Tensor(y_train) # (NUM_SAMPLES,1)
    y_test = torch.Tensor(y_test) # (NUM_SAMPLES,1)

    return X_train, X_test, y_train, y_test


# Define a Vanilla RNN layer by hand
class RNNLayer(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(RNNLayer, self).__init__()
        self.hidden_size = hidden_size
        self.input_size = input_size
        self.W_xh = nn.Parameter(torch.randn(input_size, hidden_size) * 0.01)
        self.W_hh = nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.01)
        self.activation = torch.tanh

    def forward(self, x, hidden):
        hidden = self.activation(x @ self.W_xh + hidden @ self.W_hh)
        return hidden

# Define a sequence prediction model using the Vanilla RNN
class SequenceModel(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SequenceModel, self).__init__()
        self.hidden_size = hidden_size
        self.rnn = RNNLayer(input_size, hidden_size)
        self.linear = nn.Linear(hidden_size, output_size)

    def forward(self, input_seq, seq_lengths):
        batch_size = len(input_seq)
        last_hidden = torch.zeros(batch_size, self.hidden_size, device=device)

        for b in range(batch_size):
            hidden = torch.zeros(self.hidden_size, device=device)

            seq_length =  seq_lengths[b]

            for t in range(seq_length):
                hidden = self.rnn(input_seq[b][t], hidden)

            # Store the last hidden state in the output tensor
            last_hidden[b] = hidden

        output = self.linear(last_hidden)
        return output

# Define a sequence prediction model for fixed length sequences, BUT NO SHARED WEIGHTS
class SequenceModelFixedLen(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, seq_len):
        super(SequenceModelFixedLen, self).__init__()
        self.hidden_size = hidden_size
        self.seq_len = seq_len
        self.rnn_layers = nn.ModuleList([RNNLayer(input_size, hidden_size) for _ in range(seq_len)])
        self.linear = nn.Linear(hidden_size, output_size)

    def forward(self, input_seq, seq_lengths):
        batch_size = len(input_seq)
        last_hidden = torch.zeros(batch_size, self.hidden_size, device=device)

        for b in range(batch_size):
            hidden = torch.zeros(self.hidden_size, device=device).to(device)

            seq_length = min(self.seq_len, seq_lengths[b]) 
            for t in range(seq_length):
                hidden = self.rnn_layers[t](input_seq[b][t], hidden)

            # Store the last hidden state in the output tensor
            last_hidden[b] = hidden

        output = self.linear(last_hidden)
        return output



class PaddedModel(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, seq_len_max):
        super(PaddedModel, self).__init__()
        self.hidden_size = hidden_size
        self.seq_len_max = seq_len_max
        self.rnn_layers = nn.ModuleList([RNNLayer(input_size, hidden_size) for _ in range(seq_len_max)])
        self.linear = nn.Linear(hidden_size, output_size)

    def forward(self, padded_batch, lengths):
        B, T, _ = padded_batch.shape
        device = padded_batch.device

        hidden = [torch.zeros(self.hidden_size, device=device) for _ in range(B)]

        for t in range(T):
            for b in range(B):
                if t < lengths[b]:

                    hidden[b] = self.rnn_layers[t](padded_batch[b, t], hidden[b])

        last_hidden = torch.stack(hidden, dim=0)
        return self.linear(last_hidden)



# Define hyperparameters and other settings
input_size = 10  # Replace with the actual dimension of your input features
hidden_size = 64
output_size = 1
num_epochs = 10
learning_rate = 0.001
batch_size = 32


# load data
X_train, X_test, y_train, y_test = loadData()
device = y_train.device

# Create the model using min length input
seq_lengths = [seq.shape[0] for seq in X_train]


all_indices = np.arange(len(X_train))
np.random.shuffle(all_indices)

train_cutoff = int(0.8 * len(all_indices))
train_indices = all_indices[:train_cutoff]
val_indices   = all_indices[train_cutoff:]


X_train_split = []
for i in train_indices:
    X_train_split.append(X_train[i])
y_train_split = y_train[train_indices]


X_val_split = []
for i in val_indices:
    X_val_split.append(X_train[i])
y_val_split = y_train[val_indices]


# Training loop
def train(model, num_epochs, lr, batch_size, X_train, y_train, seq_lengths):
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)
    print("training!")
    for epoch in range(num_epochs):

        print("epoch ", epoch)

        for i in range(0, len(X_train), batch_size):
            inputs = X_train[i:i+batch_size]
            targets = y_train[i:i+batch_size]
            lengths = seq_lengths[i:i+batch_size]

            #GPU related stuff to ensure it picks the right device
            inputs  = [x.to(device) for x in inputs]
            targets = targets.to(device)

            optimizer.zero_grad()
            outputs = model(inputs, lengths)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
        MSE_val = mse_padded(model, X_val_split, y_val_split)
        print("MSE ", MSE_val)
        print(loss)
    return model

def train_padded(model, num_epochs, lr, batch_size, X_train, y_train):
    model.train()
    optimizer = optim.Adam(model.parameters(), lr=lr)
    criterion = nn.MSELoss()

    print("training padded!")

    for epoch in range(num_epochs):
        print("epoch ",epoch)
        for i in range(0, len(X_train), batch_size):
            batch = X_train[i:i+batch_size]
            targets = y_train[i:i+batch_size].to(device)

            lengths = [len(s) for s in batch]
            padded = pad_sequence(batch, batch_first=True).to(device)
            optimizer.zero_grad()
            outputs = model(padded, lengths)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
        MSE_val = mse_padded(model, X_val_split, y_val_split)
        print("Padded MSE ", MSE_val)
        print(loss.item())


def mse(model, inputs, y):
    model.eval()
    crit = nn.MSELoss()
    preds = []
    bs = 64
    lengths = []

    for x in inputs:
        lengths.append(len(x))

    for i in range(0, len(inputs), bs):

        batch = []
        for x in inputs[i:i+bs]:
            batch.append(x.to(device))

        lens  = lengths[i:i+bs]
        preds.append(model(batch, lens))

    preds = torch.cat(preds, dim=0)

    return crit(preds, y.to(device)).item()



def mse_padded(model, inputs, y):
    model.eval()
    crit = nn.MSELoss()
    preds = []
    bs = 64

    lengths = []
    for x in inputs:
        lengths.append(len(x))

    for i in range(0, len(inputs), bs):
        batch = []
        for x in inputs[i:i+bs]:
            batch.append(x.to(device))

        lens = lengths[i:i+bs]
        padded = pad_sequence(batch, batch_first=True)

        preds.append(model(padded, lens))

    preds = torch.cat(preds, dim=0)
    return crit(preds, y.to(device)).item()



# initialize and train Vanilla RNN
if __name__ == "__main__":

    X_train, X_test, y_train, y_test = loadData()

    if torch.cuda.is_available():
        device = torch.device("cuda") # pick my gpu
        print("cuda selected!")
    else:
        device = torch.device("cpu")
        print("cpu selected. no visible gpu")



    seq_lengths_tr  = [len(x) for x in X_train_split]
    seq_lengths_val = [len(x) for x in X_val_split]

    print("Vanilla RNN . . . . .")
    vanilla = SequenceModel(input_size, hidden_size, output_size).to(device)
    train_vanilla_RNN =train(vanilla, num_epochs, learning_rate, batch_size, X_train, y_train, seq_lengths)


    print ("fixed length truncated model....")


    Lmin = min(seq_lengths)
    X_train_trunc = []

    for x in X_train:
         truncated_seq = x[:Lmin]
         X_train_trunc.append(truncated_seq)

    seq_lengths_trunc = [Lmin] * len(X_train_trunc)


    trunc = SequenceModelFixedLen(input_size, hidden_size, output_size, seq_len=Lmin).to(device)
    Train_trunc = train(trunc, num_epochs, learning_rate, batch_size, X_train_trunc, y_train, seq_lengths_trunc)



    print("padded model ....")
    Lmax = max(seq_lengths)
    padded_model = PaddedModel(input_size, hidden_size, output_size, seq_len_max=Lmax).to(device)
    train_padded(padded_model, num_epochs, learning_rate, batch_size, X_train, y_train)


    print("testing each")
    vanilla_test = mse(vanilla, X_test, y_test)

    trunc_test = []

    for x in X_test:
        truncated_seq = x[:Lmin]
        trunc_test.append(truncated_seq)

    test_trunc   = mse(trunc, trunc_test, y_test)

    padded_test  = mse_padded(padded_model, X_test, y_test)

    print("vanilla test!!  ", vanilla_test , "truncated test!! " , test_trunc , "Padded Test!! ", padded_test)

print("vanilla test!!  ", vanilla_test , "truncated test!! " , test_trunc , "Padded Test!! ", padded_test)


###################################################################################################################################

# initialize and train Sequential NN fixing #timesteps to the minimum sequence length

# initialize and train Sequential NN fixing #timesteps to the maximum sequence length
# NOTE: it is OK to use torch.nn.utils.rnn.pad_sequence; make sure to set parameter batch_first correctly

```


{{< embed problem_2.ipynb#prob2 >}}


### Q5(b)

Vanilla RNN: best test MSE 0.000369; 25x better than truncated (0.00920), 20x better than padded (0.00725). Fastest convergence—MSE < 0.001 by epoch 6. Truncated plateaued = 0.0085, no further gain. Padded intermediate—trained to = 0.001. No overfitting; validation loss tracked training loss.

### Q5(c)

Vanilla RNN best, shared weights, true sequence lengths are likley the reason. Truncated worst, cuts data beyond min length, loses long-range info. Padded moderate, keeps data but zero-padding adds noise. Per-timestep RNNs handle uneven inputs. Truncated/padded models larger—separate weights per timestep, harder to train with limited data.


---

# 3 Fine-tune a DistilBERT model [20 points + 2 bonus points]

In this project, you will first train a classification head using a pre-trained DistilBERT model on a dataset of social media tweets to classify tweets as containing medical information or not. You are provided with a dataset of social media tweets, where each tweet is labeled as either containing medical information (class 1) or not containing medical information (class 0).

The preprocessing of the dataset, by tokenizing the tweets and converting them into a format suitable for DistilBERT, is already provided in the starter code:
[https://colab.research.google.com/drive/17syAcTav5Wtq-n_Rs3P1cQ10szIjLlQS?usp=sharing](https://colab.research.google.com/drive/17syAcTav5Wtq-n_Rs3P1cQ10szIjLlQS?usp=sharing)

Useful documentation for this question can be found here:
[https://huggingface.co/docs/transformers/index](https://huggingface.co/docs/transformers/index)
[https://huggingface.co/docs/transformers/training](https://huggingface.co/docs/transformers/training)
[https://huggingface.co/docs/transformers/tasks/sequence_classification](https://huggingface.co/docs/transformers/tasks/sequence_classification)

You will need to make the following changes to the existing code:

Q1: (2 points) Add 4+ relevant arguments to the parser.
Hint: Check how args is used within load_pretrained_and_finetune and think about which other arguments should be added.
Note: while argparse is designed to read arguments from the command line, it is currently adapted to work with a jupyter notebook by passing arguments to parser.parse_args as a list.

Q2: (2 points) Split the code in train, val and eval (test) sets stratified by classes.

Q3: (2 points) Convert sets to HuggingFace Dataset and tokenize using function tokenize_batch.

Q4: (6 points) Implement grid search for at least one hyperparameter by training a classification head for pre-trained DistilBERT model on the training set and evaluate its performance on the validation set. You can use DistilBertForSequenceClassification.from_pretrained from the transformers library to load the pre-trained model.
Note: DO NOT train the entire model, only the classification head. You can initially freeze all parameters except the classification head by setting requires_grad=False for all parameters in the base model.

Q5: (6 points) Run the final training on train+val with best hyperparameters.

Q6: (2 points) Based on the item above, discuss the performance of the model, any challenges faced during fine-tuning, and potential improvements that can be made to further improve accuracy.

Q7: (BONUS: 2 points) Apply a principled change to your code in order to achieve F1-macro > 0.50. Explain what you did and why you did it.

---

## Answer

```python
# connect to google drive
from google.colab import drive
drive.mount('/content/drive/')

path = '/content/drive/MyDrive/'


import argparse
import logging
import os
import re
from typing import Dict
import itertools
import requests

import numpy as np
import pandas as pd
import nltk
import torch
import torch.nn as nn
from datasets import Dataset
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import TweetTokenizer
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from sklearn.model_selection import train_test_split
from transformers import (
    DistilBertForSequenceClassification,
    DistilBertTokenizerFast,
    Trainer,
    TrainingArguments,
)
from torchvision.ops import sigmoid_focal_loss


import sys
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))

import os
os.chdir(path)

def download_data(args):
    """Checks if data file exists and downloads it if not."""
    file_path = os.path.join(args.data_dir, 'Twitter.csv')
    url = 'https://raw.githubusercontent.com/LCS2-IIITD/LESA-EACL-2021/main/data/Twitter.csv'

    # Check if data_dir exists, if not, make folder
    if not os.path.exists(args.data_dir):
        os.makedirs(args.data_dir)
        logger.info('Created directory: %s', args.data_dir)

    if not os.path.exists(file_path):
        logger.info('Downloading Twitter.csv from %s', url)
        try:
            r = requests.get(url, allow_redirects=True)
            with open(file_path, 'wb') as f:
                f.write(r.content)
            logger.info('Downloaded Twitter.csv to %s', file_path)
        except requests.exceptions.RequestException as e:
            logger.error('Error downloading file: %s', e)
            raise

    args.data_path = file_path

def preprocess(df: pd.DataFrame) -> pd.DataFrame:
    """Apply light preprocessing and add `clean_text` column."""
    df = df.copy()
    df['hashtag'] = df['tweet_text'].apply(lambda x: re.findall(r'#(\w+)', str(x)))
    df['clean_text'] = df['tweet_text'].apply(lambda x: re.sub(r'http\S+|www\S+|@[\S]+', '', str(x)))

    tokenizer = TweetTokenizer()
    lemmatizer = WordNetLemmatizer()
    stop_words = set(stopwords.words('english'))

    def tok_and_clean(text: str):
        toks = tokenizer.tokenize(text.lower())
        toks = [lemmatizer.lemmatize(t) for t in toks if t.isalpha()]
        toks = [t for t in toks if t not in stop_words]
        return ' '.join(toks)

    df['clean_text'] = df['clean_text'].apply(tok_and_clean)
    return df

def compute_metrics(pred) -> Dict[str, float]:
    logits, labels = pred
    # handle logits shape (N,1) or (N,)
    logits = np.asarray(logits)
    if logits.ndim > 1:
        logits = logits.reshape(-1)
    probs = 1.0 / (1.0 + np.exp(-logits))
    preds = (probs >= 0.5).astype(int)
    acc = accuracy_score(labels, preds)
    precision_arr, recall_arr, f1_arr, _ = precision_recall_fscore_support(labels, preds, labels=[0, 1], zero_division=0)

    metrics = {
        'accuracy': float(acc),
        'precision_class_0': float(precision_arr[0]),
        'precision_class_1': float(precision_arr[1]),
        'recall_class_0': float(recall_arr[0]),
        'recall_class_1': float(recall_arr[1]),
        'f1_class_0': float(f1_arr[0]),
        'f1_class_1': float(f1_arr[1]),
    }
    metrics['f1'] = float(np.mean([metrics['f1_class_0'], metrics['f1_class_1']]))
    return metrics


#just here for troubleshooting
def compute_metrics_debug(eval_pred) -> Dict[str, float]:

    if isinstance(eval_pred, tuple):
        predictions, labels = eval_pred
    else:
        predictions, labels = eval_pred.predictions, eval_pred.label_ids


    if isinstance(predictions, tuple):
        predictions = predictions[0]

    predictions = np.asarray(predictions)
    labels = np.asarray(labels).reshape(-1)

    if predictions.ndim == 2 and predictions.shape[-1] > 1:
        preds = np.argmax(predictions, axis=-1)
    else:
        logits_1d = predictions.reshape(-1)
        probs = 1.0 / (1.0 + np.exp(-logits_1d))
        preds = (probs >= 0.5).astype(int)


    assert preds.shape[0] == labels.shape[0], f"preds {preds.shape} vs labels {labels.shape}"

    acc = accuracy_score(labels, preds)
    p, r, f1, _ = precision_recall_fscore_support(
        labels, preds, labels=[0, 1], average='macro', zero_division=0
    )
    return {
        'accuracy': float(acc),
        'precision': float(p),
        'recall': float(r),
        'f1': float(f1),
    }






def load_pretrained_and_finetune(args):
    nltk.download('stopwords')
    nltk.download('wordnet')

    os.makedirs(args.output_dir, exist_ok=True)

    logger.info('Loading data from %s', args.data_path)
    df = pd.read_csv(args.data_path)
    if 'tweet_text' not in df.columns or 'claim' not in df.columns:
        raise ValueError("Input CSV must contain 'tweet_text' and 'claim' columns")

    # print(df.head(10)) # uncomment to print first 10 rows
    df = preprocess(df)
    df = df[['clean_text', 'claim']].rename(columns={'claim': 'labels'})

    # Q2: split the code in train, val and eval (test) sets stratified by classes
    # BEGIN YOUR CODE HERE (~2 lines)

    train_val_df, test_df = train_test_split(
    df, test_size=args.test_size, stratify=df['labels'], random_state=args.seed)
    train_df, val_df = train_test_split(train_val_df,test_size=args.val_size / (1.0 - args.test_size),stratify=train_val_df['labels'],random_state=args.seed)
    
    # END YOUR CODE HERE

    tokenizer = DistilBertTokenizerFast.from_pretrained(args.model_name)

    def tokenize_batch(batch):
        return tokenizer(batch['clean_text'], truncation=True, padding='max_length', max_length=args.max_length)

    # Q3: Convert sets to HuggingFace Dataset and tokenize using function tokenize_batch
    # Use variable names: train_ds, val_ds, eval_ds
    # BEGIN YOUR CODE HERE (~6 lines)

    train_ds = Dataset.from_pandas(train_df.reset_index(drop=True))
    val_ds   = Dataset.from_pandas(val_df.reset_index(drop=True))
    eval_ds  = Dataset.from_pandas(test_df.reset_index(drop=True))  # "eval_ds" = test split

    train_ds = train_ds.map(tokenize_batch, batched=True)
    val_ds   = val_ds.map(tokenize_batch, batched=True)
    eval_ds  = eval_ds.map(tokenize_batch, batched=True)

    # keep only the tensor columns the Trainer needs
    keep_cols = ['input_ids', 'attention_mask', 'labels']
    train_ds = train_ds.remove_columns([c for c in train_ds.column_names if c not in keep_cols]).with_format('torch')
    val_ds   = val_ds.remove_columns([c for c in val_ds.column_names if c not in keep_cols]).with_format('torch')
    eval_ds  = eval_ds.remove_columns([c for c in eval_ds.column_names if c not in keep_cols]).with_format('torch')



    # END YOUR CODE HERE

    # Kepe only the necessary columns in each dataset
    #keep_cols = ['input_ids', 'attention_mask', 'labels']
    #train_ds = train_ds.remove_columns([c for c in train_ds.column_names if c not in keep_cols]).with_format('torch')
    #val_ds = val_ds.remove_columns([c for c in val_ds.column_names if c not in keep_cols]).with_format('torch')
    #test_ds = test_ds.remove_columns([c for c in test_ds.column_names if c not in keep_cols]).with_format('torch')

    # We'll create models inside the grid loop; keep tokenizer ready

    # Device selection: prefer CUDA, then Apple MPS, then CPU
    if hasattr(torch, 'cuda') and torch.cuda.is_available():
        device = 'cuda'
    else:
        try:
            if getattr(torch, 'has_mps', False) and torch.backends.mps.is_available():
                device = 'mps'
            else:
                device = 'cpu'
        except Exception:
            device = 'cpu'

    no_cuda = False if device in ('cuda', 'mps') else True

    # Note: model instances are created per-trial inside the grid search loop below.

    # Grid search over learning rate (no focal gamma needed)
    best_score = -float('inf')
    best_params = None

    # Q4: Implement grid search for at least one hyperparameter

    # Build hyperparameter lists
    best_params = None
    best_score = -float('inf')

    if args.grid_search:
        lrs = [0.001, 0.0001]
        bss = [8,32]
        wds = [0.0, 0.03]
        nes = [1, 3]
        search_space = itertools.product(lrs, bss, wds, nes)
    else:
        search_space = [(args.learning_rate, args.batch_size, args.weight_decay, args.num_train_epochs)]

    # BEGIN YOUR CODE HERE (~1-7 lines)


    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


    # PRO TIP: you can use itertools.product to create a master list of all combos for grid search

    for lr, bs, wd, ne in search_space:# COMPLETE THIS LINE
        logger.info(f"Trial: lr={lr}, bs={bs}, wd={wd}, epochs={ne}")

        # instantiate fresh pretrained model for each trial
        # NOTE: if num_labels=2, CrossEntropy is assumed by default Trainer;
        # if num_labels=1, you will have to implement a CustomTrainer to override MSE as loss
        model = DistilBertForSequenceClassification.from_pretrained(
        args.model_name, num_labels=2
    )

        # Freeze base params and unfreeze classifier layers
        for name, p in model.named_parameters():
            p.requires_grad = False
        for name, p in model.named_parameters():
            if name.startswith('classifier') or 'pre_classifier' in name:
                p.requires_grad = True

        # trial output dir
        trial_output_dir = os.path.join(args.output_dir, f'CHOOSE_EXP_NAME')
        os.makedirs(trial_output_dir, exist_ok=True)

        training_args = TrainingArguments(
            output_dir=trial_output_dir,
            per_device_train_batch_size=bs,
            per_device_eval_batch_size=bs,
            learning_rate=lr,
            num_train_epochs=ne,
            weight_decay=wd,
            logging_dir=os.path.join(trial_output_dir, "logs"),  # safe on old & new
        )

        # initialize the Trainer (~5 lines) or CustomTrainer (~6-8 lines)
        trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_ds,
        eval_dataset=val_ds,
        compute_metrics=compute_metrics_debug
     # COMPLETE THIS LINE
        )
        model.to(device)
        # move model to device if possible
        # BEGIN YOUR CODE HERE (~1-4 lines)
        trainer.train()
        metrics = trainer.evaluate(eval_dataset=val_ds)
        score = metrics.get('eval_f1', -float('inf'))


        # END YOUR CODE HERE

        # train, evaluate, compute score, update best score
        # BEGIN YOUR CODE HERE (~6-9 lines)
        if score > best_score:
            best_score = score
            best_params = {'lr': lr, 'bs': bs, 'wd': wd, 'epochs': ne}
            print()
        # END YOUR CODE HERE

    # END YOUR CODE HERE

    if best_params is None:
        raise RuntimeError('Grid search failed to produce any candidate best params')

    logger.info(best_params)

        # Q5: Final training on train+val with best hyperparameters
    # instantiate pretrained model and freeze appropriate layers
    # BEGIN YOUR CODE HERE (~6 lines)
    model = DistilBertForSequenceClassification.from_pretrained(args.model_name, num_labels=2)

    for name, p in model.named_parameters():
        p.requires_grad = False
    for name, p in model.named_parameters():
        if name.startswith('classifier') or 'pre_classifier' in name:
            p.requires_grad = True

    # END YOUR CODE HE RE

    # combine train_ds and val_df for final training (i.e., original train_df)
    # BEGIN YOUR CODE HERE (~4 lines)
    from datasets import concatenate_datasets
    trainval_ds = concatenate_datasets([train_ds, val_ds]).with_format('torch')
    final_out_dir = os.path.join(args.output_dir, 'final')
    os.makedirs(final_out_dir, exist_ok=True)
    # END YOUR CODE HERE

    # set the training arguments
    # BEGIN YOUR CODE HERE (~9-14 lines)
    final_args = TrainingArguments(
        output_dir=final_out_dir,
        per_device_train_batch_size=best_params['bs'],
        per_device_eval_batch_size=best_params['bs'],
        learning_rate=best_params['lr'],
        num_train_epochs=best_params['epochs'],
        weight_decay=best_params['wd'],
        logging_dir=os.path.join(final_out_dir, "logs"),
    )
    # END YOUR CODE HERE

    # initialize the Trainer (~5 lines) or CustomTrainer (~6-8 lines)
    final_trainer = Trainer(
        model=model,
        args=final_args,
        train_dataset=trainval_ds,
        eval_dataset=eval_ds,          # held-out test split
        compute_metrics=compute_metrics_debug
    )

    final_trainer.train()
    test_metrics = final_trainer.evaluate(eval_dataset=eval_ds)
    logger.info(f'Final TEST metrics: {test_metrics}')
    print("FINAL TEST METRICS (Q5 Results):")
    for key, value in test_metrics.items():
      print(f"{key}: {value:.4f}")
    
    final_trainer.save_model(args.output_dir)
    # move model to device if possible
    # BEGIN YOUR CODE HERE (~1-4 lines)
    # END YOUR CODE HERE

    # train, evaluate, save model
    # BEGIN YOUR CODE HERE (~3 lines)
    # END YOUR CODE HERE
    trainer.train()
    trainer.save_model(args.output_dir)
    return test_metrics




# if __name__ == '__main__':
parser = argparse.ArgumentParser()
parser.add_argument('--data-dir', type=str, required=True, help='Path to CSV with tweet_text and claim columns')
parser.add_argument('--output-dir', type=str, default='finetuned', help='Where to save model and tokenizer')
parser.add_argument('--model-name', type=str, default='distilbert-base-uncased', help='Pretrained model name')
# Q1. Add relevant arguments


parser.add_argument('--max-length', type=int, default=128, help='max number of tolkens')
parser.add_argument('--num-train-epochs', type=int, default=3, help='total number of epochs to run through')
parser.add_argument('--weight-decay', type=float, default=0.01, help='weight decay strength')
parser.add_argument('--grid-search', action='store_true', help='do a grid search with hyperparameters')
parser.add_argument('--val-size', type=float, default=0.15, help='portion of data reserved for validation set')
parser.add_argument('--test-size', type=float, default=0.15, help='portion of data reserved for tests')
parser.add_argument('--seed', type=int, default=42, help='Random seed')
parser.add_argument('--eval-steps', type=int, default=100, help='evaluate evcery n amount of steps')
parser.add_argument('--logging-steps', type=int, default=50, help='how loften to log metrics')
parser.add_argument('--batch-size', type=int, default=16, help='number of samples processed per device')
parser.add_argument('--learning-rate', type=float, default=2e-5, help='learning rate for the optimizer')


# BEGIN YOUR CODE HERE (~5-15 lines)
# END YOUR CODE HERE

args = parser.parse_args(['--data-dir', 'data', '--grid-search'])

print(args)

# download file if it doesn't exist yet
download_data(args)

# load pre-trained and finetune
test_metrics = load_pretrained_and_finetune(args)



#| label: prob3

accuracy = test_metrics.get('eval_accuracy', 0)
f1 = test_metrics.get('eval_f1', 0)
precision = test_metrics.get('eval_precision', 0)
recall = test_metrics.get('eval_recall', 0)
print(f"MODEL PERFORMANCE:")
print(f"   Accuracy: {accuracy:.2%}")
print(f"   F1 Score: {f1:.4f}")
print(f"   Precision: {precision:.4f}")
print(f"   Recall: {recall:.4f}")

```

{{< embed problem_3.ipynb#prob3 >}}

### Q6

Accuracy 87.45%, above 60% target. F1 = 0.4769 (<0.50) due to imbalance between precision (0.7708) and recall (0.5049). High precision, low recall meaning conservative positive predictions; catches true medical tweets but misses many. Common in imbalanced datasets with dominant negative class.

Challenges: strong class imbalance (few medical tweets), noisy text (hashtags, slang, abbreviations), limited training (3 epochs per grid search config). Only classification head trained—base model frozen, limiting domain adaptation.

Improvements: add class weights, use focal loss, train longer (5–10 epochs), unfreeze more layers, augment minority data, tune threshold (<0.5), or switch to larger model (e.g., BERT). Grid search tested 16 setups; best config used for final combined train+val training.

---


# 4 NOT PART OF HW4: Tensor Shapes in a Transformer Layer [0 points]


The figura above shows the transformer layer. The input size of the transformer layer is [10, 90, 20] (where 10 represents batch size, 90 represents sequence length, and 20 represents hidden size). We consider 5 attention heads in this attention layer. The shape of the (combined) projection matrices is $H\times H$, which are used to project the input data to Q (query), K (key), V (value). Please compute the size of the each output, including:
1. The shape of Q
2. The shape of K
3. The shape of V
4. The shape of Q for each head
5. The shape of K for each head
6. The shape of V for each head
7. The shape of the attention map (output of softmax)
8. The shape of Dropout-1's output
9. The shape of Output
10. The shape of Dropout-2's output
11. Total number of the parameters in this transformer layer

For this homework, you are encouraged to experiment with various models and training strategies. To achieve a full score, your model must achieve at least 60% accuracy on the test set.

---

# Submission

Submit one PDF file that includes your notes for the theoretical problems (scanned or typed) and screenshots of your code for the programming problems. All material in the submitted PDF must be presented in a clear and readable format.
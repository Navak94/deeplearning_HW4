% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrartcl}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{fvextra}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}

\usepackage{multicol}
\newcommand{\btwocol}{\begin{multicols}{2}}
\newcommand{\etwocol}{\end{multicols}}
\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={CS/DS 541: Deep Learning},
  pdfauthor={Tom Arnold \& Nate Hindman},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{CS/DS 541: Deep Learning}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Homework 4}
\author{Tom Arnold \& Nate Hindman}
\date{}

\begin{document}
\maketitle

\RecustomVerbatimEnvironment{verbatim}{Verbatim}{
  showspaces = false,
  showtabs = false,
  breaksymbolleft={},
  breaklines
  % Note: setting commandchars=\\\{\} here will cause an error
}


\btwocol

\emph{Due: 5:59pm ET Thursday October 9}

\emph{This problem can be done in teams of up 2 students.}

\etwocol

\section{1 Window Type Classification {[}20
points{]}}\label{window-type-classification-20-points}

In this problem, the goal is to classify window images into one of five
categories: ``New Awning Window'', ``New Bay Window'', ``New Fixed
Window'', ``New Horizontal Sliding Window'', and ``New Hung Window''.

The training and test datasets can be accessed via the following links:
* \url{https://canvas.wpi.edu/files/7719816/download?download_frd=1} and
* \url{https://canvas.wpi.edu/files/7719811/download?download_frd=1}.

To help you get started, a demo code are available at:
\url{https://colab.research.google.com/drive/1fG0f6LiPnv7a4nDWNPtVyo1Y0Xh5vp71?usp=sharing}.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Answer}\label{answer}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# import packages}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ torch}
\ImportTok{import}\NormalTok{ json}
\NormalTok{device }\OperatorTok{=}\NormalTok{ torch.device(}\StringTok{\textquotesingle{}cuda\textquotesingle{}} \ControlFlowTok{if}\NormalTok{ torch.cuda.is\_available() }\ControlFlowTok{else} \StringTok{\textquotesingle{}cpu\textquotesingle{}}\NormalTok{)}


\CommentTok{\# connect to google drive}
\ImportTok{from}\NormalTok{ google.colab }\ImportTok{import}\NormalTok{ drive}
\NormalTok{drive.mount(}\StringTok{\textquotesingle{}/content/drive/\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Step 1}

\CommentTok{\# NEW TOM + NATE BLOCK}
\ImportTok{import}\NormalTok{ torch.nn }\ImportTok{as}\NormalTok{ nn}

\KeywordTok{class}\NormalTok{ betterCNN(nn.Module):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, num\_classes}\OperatorTok{=}\DecValTok{5}\NormalTok{):}
        \BuiltInTok{super}\NormalTok{(betterCNN, }\VariableTok{self}\NormalTok{).}\FunctionTok{\_\_init\_\_}\NormalTok{()}

        \CommentTok{\# convolutional block}
        \VariableTok{self}\NormalTok{.conv1 }\OperatorTok{=}\NormalTok{ nn.Conv2d(}\DecValTok{3}\NormalTok{, }\DecValTok{64}\NormalTok{, kernel\_size}\OperatorTok{=}\DecValTok{3}\NormalTok{, stride}\OperatorTok{=}\DecValTok{1}\NormalTok{, padding}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
        \VariableTok{self}\NormalTok{.bn1 }\OperatorTok{=}\NormalTok{ nn.BatchNorm2d(}\DecValTok{64}\NormalTok{)}
        \VariableTok{self}\NormalTok{.conv2 }\OperatorTok{=}\NormalTok{ nn.Conv2d(}\DecValTok{64}\NormalTok{, }\DecValTok{64}\NormalTok{, kernel\_size}\OperatorTok{=}\DecValTok{3}\NormalTok{, stride}\OperatorTok{=}\DecValTok{1}\NormalTok{, padding}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
        \VariableTok{self}\NormalTok{.bn2 }\OperatorTok{=}\NormalTok{ nn.BatchNorm2d(}\DecValTok{64}\NormalTok{)}

        \CommentTok{\# convolutional block}
        \VariableTok{self}\NormalTok{.conv3 }\OperatorTok{=}\NormalTok{ nn.Conv2d(}\DecValTok{64}\NormalTok{, }\DecValTok{128}\NormalTok{, kernel\_size}\OperatorTok{=}\DecValTok{3}\NormalTok{, stride}\OperatorTok{=}\DecValTok{1}\NormalTok{, padding}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
        \VariableTok{self}\NormalTok{.bn3 }\OperatorTok{=}\NormalTok{ nn.BatchNorm2d(}\DecValTok{128}\NormalTok{)}
        \VariableTok{self}\NormalTok{.conv4 }\OperatorTok{=}\NormalTok{ nn.Conv2d(}\DecValTok{128}\NormalTok{, }\DecValTok{128}\NormalTok{, kernel\_size}\OperatorTok{=}\DecValTok{3}\NormalTok{, stride}\OperatorTok{=}\DecValTok{1}\NormalTok{, padding}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
        \VariableTok{self}\NormalTok{.bn4 }\OperatorTok{=}\NormalTok{ nn.BatchNorm2d(}\DecValTok{128}\NormalTok{)}

        \CommentTok{\# convolutional block}
        \VariableTok{self}\NormalTok{.conv5 }\OperatorTok{=}\NormalTok{ nn.Conv2d(}\DecValTok{128}\NormalTok{, }\DecValTok{256}\NormalTok{, kernel\_size}\OperatorTok{=}\DecValTok{3}\NormalTok{, stride}\OperatorTok{=}\DecValTok{1}\NormalTok{, padding}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
        \VariableTok{self}\NormalTok{.bn5 }\OperatorTok{=}\NormalTok{ nn.BatchNorm2d(}\DecValTok{256}\NormalTok{)}
        \VariableTok{self}\NormalTok{.conv6 }\OperatorTok{=}\NormalTok{ nn.Conv2d(}\DecValTok{256}\NormalTok{, }\DecValTok{256}\NormalTok{, kernel\_size}\OperatorTok{=}\DecValTok{3}\NormalTok{, stride}\OperatorTok{=}\DecValTok{1}\NormalTok{, padding}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
        \VariableTok{self}\NormalTok{.bn6 }\OperatorTok{=}\NormalTok{ nn.BatchNorm2d(}\DecValTok{256}\NormalTok{)}

        \CommentTok{\# pooling and dropout}
        \VariableTok{self}\NormalTok{.pool }\OperatorTok{=}\NormalTok{ nn.MaxPool2d(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)}
        \VariableTok{self}\NormalTok{.dropout }\OperatorTok{=}\NormalTok{ nn.Dropout(}\FloatTok{0.2}\NormalTok{)}

        \CommentTok{\# fully connected layers}
        \VariableTok{self}\NormalTok{.fc1 }\OperatorTok{=}\NormalTok{ nn.Linear(}\DecValTok{256} \OperatorTok{*} \DecValTok{8} \OperatorTok{*} \DecValTok{8}\NormalTok{, }\DecValTok{512}\NormalTok{)}
        \VariableTok{self}\NormalTok{.fc2 }\OperatorTok{=}\NormalTok{ nn.Linear(}\DecValTok{512}\NormalTok{, num\_classes)}

    \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, x):}
        \CommentTok{\# 1 block}
\NormalTok{        x }\OperatorTok{=} \VariableTok{self}\NormalTok{.conv1(x)}
\NormalTok{        x }\OperatorTok{=} \VariableTok{self}\NormalTok{.bn1(x)}
\NormalTok{        x }\OperatorTok{=}\NormalTok{ torch.relu(x)}
\NormalTok{        x }\OperatorTok{=} \VariableTok{self}\NormalTok{.conv2(x)}
\NormalTok{        x }\OperatorTok{=} \VariableTok{self}\NormalTok{.bn2(x)}
\NormalTok{        x }\OperatorTok{=}\NormalTok{ torch.relu(x)}
\NormalTok{        x }\OperatorTok{=} \VariableTok{self}\NormalTok{.pool(x)}

        \CommentTok{\# 2 block}
\NormalTok{        x }\OperatorTok{=} \VariableTok{self}\NormalTok{.conv3(x)}
\NormalTok{        x }\OperatorTok{=} \VariableTok{self}\NormalTok{.bn3(x)}
\NormalTok{        x }\OperatorTok{=}\NormalTok{ torch.relu(x)}
\NormalTok{        x }\OperatorTok{=} \VariableTok{self}\NormalTok{.conv4(x)}
\NormalTok{        x }\OperatorTok{=} \VariableTok{self}\NormalTok{.bn4(x)}
\NormalTok{        x }\OperatorTok{=}\NormalTok{ torch.relu(x)}
\NormalTok{        x }\OperatorTok{=} \VariableTok{self}\NormalTok{.pool(x)}

        \CommentTok{\# 3 block}
\NormalTok{        x }\OperatorTok{=} \VariableTok{self}\NormalTok{.conv5(x)}
\NormalTok{        x }\OperatorTok{=} \VariableTok{self}\NormalTok{.bn5(x)}
\NormalTok{        x }\OperatorTok{=}\NormalTok{ torch.relu(x)}
\NormalTok{        x }\OperatorTok{=} \VariableTok{self}\NormalTok{.conv6(x)}
\NormalTok{        x }\OperatorTok{=} \VariableTok{self}\NormalTok{.bn6(x)}
\NormalTok{        x }\OperatorTok{=}\NormalTok{ torch.relu(x)}
\NormalTok{        x }\OperatorTok{=} \VariableTok{self}\NormalTok{.pool(x)}

        \CommentTok{\# flatten}
\NormalTok{        x }\OperatorTok{=}\NormalTok{ x.view(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{256} \OperatorTok{*} \DecValTok{8} \OperatorTok{*} \DecValTok{8}\NormalTok{)}

        \CommentTok{\# fully connected layers}
\NormalTok{        x }\OperatorTok{=} \VariableTok{self}\NormalTok{.fc1(x)}
\NormalTok{        x }\OperatorTok{=}\NormalTok{ torch.relu(x)}
\NormalTok{        x }\OperatorTok{=} \VariableTok{self}\NormalTok{.dropout(x)}
\NormalTok{        x }\OperatorTok{=} \VariableTok{self}\NormalTok{.fc2(x)}

        \ControlFlowTok{return}\NormalTok{ x}

\NormalTok{model }\OperatorTok{=}\NormalTok{ betterCNN(num\_classes}\OperatorTok{=}\DecValTok{5}\NormalTok{)}
\NormalTok{model }\OperatorTok{=}\NormalTok{ model.to(device)}
\BuiltInTok{print}\NormalTok{(model)}

\CommentTok{\# Step 2}

\CommentTok{\# NEW TOM + NATE BLOCK}

\ImportTok{from}\NormalTok{ torchvision }\ImportTok{import}\NormalTok{ datasets, transforms}
\ImportTok{from}\NormalTok{ torch.utils.data }\ImportTok{import}\NormalTok{ DataLoader, random\_split}

\CommentTok{\# transform with augmentation}
\NormalTok{train\_transform }\OperatorTok{=}\NormalTok{ transforms.Compose([}
\NormalTok{    transforms.Resize((}\DecValTok{64}\NormalTok{, }\DecValTok{64}\NormalTok{)),}
\NormalTok{    transforms.RandomHorizontalFlip(p}\OperatorTok{=}\FloatTok{0.5}\NormalTok{),}
\NormalTok{    transforms.RandomRotation(degrees}\OperatorTok{=}\DecValTok{15}\NormalTok{),}
\NormalTok{    transforms.ColorJitter(brightness}\OperatorTok{=}\FloatTok{0.2}\NormalTok{, contrast}\OperatorTok{=}\FloatTok{0.2}\NormalTok{, saturation}\OperatorTok{=}\FloatTok{0.2}\NormalTok{, hue}\OperatorTok{=}\FloatTok{0.1}\NormalTok{),}
\NormalTok{    transforms.RandomAffine(degrees}\OperatorTok{=}\DecValTok{0}\NormalTok{, translate}\OperatorTok{=}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.1}\NormalTok{)),}
\NormalTok{    transforms.ToTensor(),}
\NormalTok{    transforms.Normalize((}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{), (}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{))}
\NormalTok{])}

\CommentTok{\# val transform (no augmentation)}
\NormalTok{val\_transform }\OperatorTok{=}\NormalTok{ transforms.Compose([}
\NormalTok{    transforms.Resize((}\DecValTok{64}\NormalTok{, }\DecValTok{64}\NormalTok{)),}
\NormalTok{    transforms.ToTensor(),}
\NormalTok{    transforms.Normalize((}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{), (}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{))}
\NormalTok{])}

\NormalTok{trainset\_dir }\OperatorTok{=} \StringTok{\textquotesingle{}/content/drive/MyDrive/train\_set\textquotesingle{}}

\NormalTok{full\_dataset }\OperatorTok{=}\NormalTok{ datasets.ImageFolder(root}\OperatorTok{=}\NormalTok{trainset\_dir, transform}\OperatorTok{=}\VariableTok{None}\NormalTok{)}

\NormalTok{total\_size }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(full\_dataset)}
\NormalTok{indices }\OperatorTok{=} \BuiltInTok{list}\NormalTok{(}\BuiltInTok{range}\NormalTok{(total\_size))}
\NormalTok{np.random.seed(}\DecValTok{42}\NormalTok{)}
\NormalTok{np.random.shuffle(indices)}

\NormalTok{train\_size }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(}\FloatTok{0.85} \OperatorTok{*}\NormalTok{ total\_size)}
\NormalTok{val\_size }\OperatorTok{=}\NormalTok{ total\_size }\OperatorTok{{-}}\NormalTok{ train\_size}

\NormalTok{train\_indices }\OperatorTok{=}\NormalTok{ indices[}\DecValTok{0}\NormalTok{:train\_size]}
\NormalTok{val\_indices }\OperatorTok{=}\NormalTok{ indices[train\_size:total\_size]}

\NormalTok{train\_dataset }\OperatorTok{=}\NormalTok{ datasets.ImageFolder(root}\OperatorTok{=}\NormalTok{trainset\_dir, transform}\OperatorTok{=}\NormalTok{train\_transform)}
\NormalTok{train\_subset }\OperatorTok{=}\NormalTok{ torch.utils.data.Subset(train\_dataset, train\_indices)}

\NormalTok{val\_dataset }\OperatorTok{=}\NormalTok{ datasets.ImageFolder(root}\OperatorTok{=}\NormalTok{trainset\_dir, transform}\OperatorTok{=}\NormalTok{val\_transform)}
\NormalTok{val\_subset }\OperatorTok{=}\NormalTok{ torch.utils.data.Subset(val\_dataset, val\_indices)}

\NormalTok{train\_loader }\OperatorTok{=}\NormalTok{ DataLoader(train\_subset, batch\_size}\OperatorTok{=}\DecValTok{64}\NormalTok{, shuffle}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{val\_loader }\OperatorTok{=}\NormalTok{ DataLoader(val\_subset, batch\_size}\OperatorTok{=}\DecValTok{64}\NormalTok{, shuffle}\OperatorTok{=}\VariableTok{False}\NormalTok{)}


\CommentTok{\# Step 3}

\CommentTok{\# NEW TOM + NATE BLOCK}

\ImportTok{import}\NormalTok{ torch.optim }\ImportTok{as}\NormalTok{ optim}
\NormalTok{criterion }\OperatorTok{=}\NormalTok{ nn.CrossEntropyLoss(label\_smoothing}\OperatorTok{=}\FloatTok{0.09}\NormalTok{) }\CommentTok{\# added label smoothing}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ optim.Adam(model.parameters(), lr}\OperatorTok{=}\FloatTok{0.002}\NormalTok{, weight\_decay}\OperatorTok{=}\FloatTok{0.00001}\NormalTok{) }\CommentTok{\# added weight decay}
\CommentTok{\# added learning rate scheduler}
\NormalTok{scheduler }\OperatorTok{=}\NormalTok{ optim.lr\_scheduler.ReduceLROnPlateau(}
\NormalTok{    optimizer,}
\NormalTok{    mode}\OperatorTok{=}\StringTok{\textquotesingle{}max\textquotesingle{}}\NormalTok{,}
\NormalTok{    factor}\OperatorTok{=}\FloatTok{0.5}\NormalTok{,}
\NormalTok{    patience}\OperatorTok{=}\DecValTok{5}\NormalTok{)}

\CommentTok{\# Step 4}

\CommentTok{\# NEW TOM + NATE BLOCK}

\KeywordTok{def}\NormalTok{ train(model, train\_loader, val\_loader, criterion, optimizer, scheduler, num\_epochs}\OperatorTok{=}\DecValTok{30}\NormalTok{):}
\NormalTok{    best\_val\_acc }\OperatorTok{=} \FloatTok{0.0}
\NormalTok{    patience\_counter }\OperatorTok{=} \DecValTok{0}
\NormalTok{    early\_stop\_patience }\OperatorTok{=} \DecValTok{10}

\NormalTok{    train\_losses }\OperatorTok{=}\NormalTok{ []}
\NormalTok{    val\_accuracies }\OperatorTok{=}\NormalTok{ []}

    \ControlFlowTok{for}\NormalTok{ epoch }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(num\_epochs):}
        \CommentTok{\# train}
\NormalTok{        model.train()}
\NormalTok{        running\_loss }\OperatorTok{=} \FloatTok{0.0}
\NormalTok{        train\_correct }\OperatorTok{=} \DecValTok{0}
\NormalTok{        train\_total }\OperatorTok{=} \DecValTok{0}

        \ControlFlowTok{for}\NormalTok{ inputs, labels }\KeywordTok{in}\NormalTok{ train\_loader:}
\NormalTok{            inputs }\OperatorTok{=}\NormalTok{ inputs.to(device)}
\NormalTok{            labels }\OperatorTok{=}\NormalTok{ labels.to(device)}

\NormalTok{            optimizer.zero\_grad()}
\NormalTok{            outputs }\OperatorTok{=}\NormalTok{ model(inputs)}
\NormalTok{            loss }\OperatorTok{=}\NormalTok{ criterion(outputs, labels)}
\NormalTok{            loss.backward()}
\NormalTok{            optimizer.step()}

\NormalTok{            running\_loss }\OperatorTok{=}\NormalTok{ running\_loss }\OperatorTok{+}\NormalTok{ loss.item()}

\NormalTok{            predictions }\OperatorTok{=}\NormalTok{ torch.}\BuiltInTok{max}\NormalTok{(outputs, }\DecValTok{1}\NormalTok{)[}\DecValTok{1}\NormalTok{]}
\NormalTok{            train\_total }\OperatorTok{=}\NormalTok{ train\_total }\OperatorTok{+}\NormalTok{ labels.size(}\DecValTok{0}\NormalTok{)}
\NormalTok{            train\_correct }\OperatorTok{=}\NormalTok{ train\_correct }\OperatorTok{+}\NormalTok{ (predictions }\OperatorTok{==}\NormalTok{ labels).}\BuiltInTok{sum}\NormalTok{().item()}

\NormalTok{        avg\_train\_loss }\OperatorTok{=}\NormalTok{ running\_loss }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(train\_loader)}
\NormalTok{        train\_acc }\OperatorTok{=} \FloatTok{100.0} \OperatorTok{*}\NormalTok{ train\_correct }\OperatorTok{/}\NormalTok{ train\_total}
\NormalTok{        train\_losses.append(avg\_train\_loss)}

        \CommentTok{\# val}
\NormalTok{        model.}\BuiltInTok{eval}\NormalTok{()}
\NormalTok{        val\_correct }\OperatorTok{=} \DecValTok{0}
\NormalTok{        val\_total }\OperatorTok{=} \DecValTok{0}

        \ControlFlowTok{with}\NormalTok{ torch.no\_grad():}
            \ControlFlowTok{for}\NormalTok{ inputs, labels }\KeywordTok{in}\NormalTok{ val\_loader:}
\NormalTok{                inputs }\OperatorTok{=}\NormalTok{ inputs.to(device)}
\NormalTok{                labels }\OperatorTok{=}\NormalTok{ labels.to(device)}
\NormalTok{                outputs }\OperatorTok{=}\NormalTok{ model(inputs)}
\NormalTok{                predictions }\OperatorTok{=}\NormalTok{ torch.}\BuiltInTok{max}\NormalTok{(outputs, }\DecValTok{1}\NormalTok{)[}\DecValTok{1}\NormalTok{]}
\NormalTok{                val\_total }\OperatorTok{=}\NormalTok{ val\_total }\OperatorTok{+}\NormalTok{ labels.size(}\DecValTok{0}\NormalTok{)}
\NormalTok{                val\_correct }\OperatorTok{=}\NormalTok{ val\_correct }\OperatorTok{+}\NormalTok{ (predictions }\OperatorTok{==}\NormalTok{ labels).}\BuiltInTok{sum}\NormalTok{().item()}

\NormalTok{        val\_acc }\OperatorTok{=} \FloatTok{100.0} \OperatorTok{*}\NormalTok{ val\_correct }\OperatorTok{/}\NormalTok{ val\_total}
\NormalTok{        val\_accuracies.append(val\_acc)}

        \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Epoch }\SpecialCharTok{\{}\NormalTok{epoch }\OperatorTok{+} \DecValTok{1}\SpecialCharTok{\}}\SpecialStringTok{/}\SpecialCharTok{\{}\NormalTok{num\_epochs}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
        \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f" {-}\textgreater{} Train Loss: }\SpecialCharTok{\{}\NormalTok{avg\_train\_loss}\SpecialCharTok{:.4f\}}\SpecialStringTok{, Train Acc: }\SpecialCharTok{\{}\NormalTok{train\_acc}\SpecialCharTok{:.2f\}}\SpecialStringTok{\%"}\NormalTok{)}
        \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f" {-}\textgreater{} Val Acc: }\SpecialCharTok{\{}\NormalTok{val\_acc}\SpecialCharTok{:.2f\}}\SpecialStringTok{\%"}\NormalTok{)}

        \CommentTok{\# new scheduler step}
\NormalTok{        scheduler.step(val\_acc)}

        \CommentTok{\# save state added}
        \ControlFlowTok{if}\NormalTok{ val\_acc }\OperatorTok{\textgreater{}=}\NormalTok{ best\_val\_acc:}
\NormalTok{            best\_val\_acc }\OperatorTok{=}\NormalTok{ val\_acc}
\NormalTok{            torch.save(model.state\_dict(), }\StringTok{\textquotesingle{}best\_model.pth\textquotesingle{}}\NormalTok{)}
            \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"{-}{-}{-}{-}\textgreater{} Best model saved {-}{-}{-}\textgreater{} Val Acc: }\SpecialCharTok{\{}\NormalTok{val\_acc}\SpecialCharTok{:.2f\}}\SpecialStringTok{\%"}\NormalTok{)}
\NormalTok{            patience\_counter }\OperatorTok{=} \DecValTok{0}
        \ControlFlowTok{else}\NormalTok{:}
\NormalTok{            patience\_counter }\OperatorTok{=}\NormalTok{ patience\_counter }\OperatorTok{+} \DecValTok{1}

        \CommentTok{\# early stopping}
        \ControlFlowTok{if}\NormalTok{ patience\_counter }\OperatorTok{\textgreater{}=}\NormalTok{ early\_stop\_patience:}
            \ControlFlowTok{break}

        \BuiltInTok{print}\NormalTok{()}

    \ControlFlowTok{return}\NormalTok{ train\_losses, val\_accuracies}

\NormalTok{train\_losses, val\_accuracies }\OperatorTok{=}\NormalTok{ train(}
\NormalTok{    model,}
\NormalTok{    train\_loader,}
\NormalTok{    val\_loader,}
\NormalTok{    criterion,}
\NormalTok{    optimizer,}
\NormalTok{    scheduler,}
\NormalTok{    num\_epochs}\OperatorTok{=}\DecValTok{100}
\NormalTok{)}


\CommentTok{\# Step 5}

\CommentTok{\# NEW TOM + NATE BLOCK}

\KeywordTok{def}\NormalTok{ evaluate(model, val\_loader):}
\NormalTok{    model.}\BuiltInTok{eval}\NormalTok{()}
\NormalTok{    correct }\OperatorTok{=} \DecValTok{0}
\NormalTok{    total }\OperatorTok{=} \DecValTok{0}

    \ControlFlowTok{with}\NormalTok{ torch.no\_grad():}
        \ControlFlowTok{for}\NormalTok{ inputs, labels }\KeywordTok{in}\NormalTok{ val\_loader:}
\NormalTok{            inputs }\OperatorTok{=}\NormalTok{ inputs.to(device)}
\NormalTok{            labels }\OperatorTok{=}\NormalTok{ labels.to(device)}
\NormalTok{            outputs }\OperatorTok{=}\NormalTok{ model(inputs)}
\NormalTok{            predictions }\OperatorTok{=}\NormalTok{ torch.}\BuiltInTok{max}\NormalTok{(outputs, }\DecValTok{1}\NormalTok{)[}\DecValTok{1}\NormalTok{]}
\NormalTok{            total }\OperatorTok{=}\NormalTok{ total }\OperatorTok{+}\NormalTok{ labels.size(}\DecValTok{0}\NormalTok{)}
\NormalTok{            correct }\OperatorTok{=}\NormalTok{ correct }\OperatorTok{+}\NormalTok{ (predictions }\OperatorTok{==}\NormalTok{ labels).}\BuiltInTok{sum}\NormalTok{().item()}

\NormalTok{    accuracy }\OperatorTok{=} \FloatTok{100.0} \OperatorTok{*}\NormalTok{ correct }\OperatorTok{/}\NormalTok{ total}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f\textquotesingle{}Val Accuracy: }\SpecialCharTok{\{}\NormalTok{accuracy}\SpecialCharTok{:.2f\}}\SpecialStringTok{\%\textquotesingle{}}\NormalTok{)}
    \ControlFlowTok{return}\NormalTok{ accuracy}

\NormalTok{model.load\_state\_dict(torch.load(}\StringTok{\textquotesingle{}best\_model.pth\textquotesingle{}}\NormalTok{))}
\NormalTok{evaluate(model, val\_loader)}

\NormalTok{Please don}\StringTok{\textquotesingle{}t make any change after this line. The only parameters you may modify are those within the "test\_transform" function.}

\ErrorTok{import os}
\ImportTok{from}\NormalTok{ PIL }\ImportTok{import}\NormalTok{ Image}
\KeywordTok{class}\NormalTok{ CustomImageDataset(torch.utils.data.Dataset):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, folder\_path, transform}\OperatorTok{=}\VariableTok{None}\NormalTok{):}
        \VariableTok{self}\NormalTok{.folder\_path }\OperatorTok{=}\NormalTok{ folder\_path}
        \VariableTok{self}\NormalTok{.transform }\OperatorTok{=}\NormalTok{ transform}
        \VariableTok{self}\NormalTok{.image\_paths }\OperatorTok{=}\NormalTok{ [os.path.join(folder\_path, filename) }\ControlFlowTok{for}\NormalTok{ filename }\KeywordTok{in}\NormalTok{ os.listdir(folder\_path) }\ControlFlowTok{if}\NormalTok{ filename.endswith((}\StringTok{\textquotesingle{}png\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}jpg\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}jpeg\textquotesingle{}}\NormalTok{))]}

    \KeywordTok{def} \FunctionTok{\_\_len\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{):}
        \ControlFlowTok{return} \BuiltInTok{len}\NormalTok{(}\VariableTok{self}\NormalTok{.image\_paths)}

    \KeywordTok{def}\NormalTok{ filename2index(}\VariableTok{self}\NormalTok{, filename):}
        \ControlFlowTok{return}\NormalTok{ os.path.basename(filename).replace(}\StringTok{\textquotesingle{}.jpg\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{)}

    \KeywordTok{def} \FunctionTok{\_\_getitem\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, idx):}
\NormalTok{        img\_path }\OperatorTok{=} \VariableTok{self}\NormalTok{.image\_paths[idx]}
\NormalTok{        img }\OperatorTok{=}\NormalTok{ Image.}\BuiltInTok{open}\NormalTok{(img\_path).convert(}\StringTok{\textquotesingle{}RGB\textquotesingle{}}\NormalTok{)}
        \ControlFlowTok{if} \VariableTok{self}\NormalTok{.transform:}
\NormalTok{            img }\OperatorTok{=} \VariableTok{self}\NormalTok{.transform(img)}
        \ControlFlowTok{return}\NormalTok{ img, }\VariableTok{self}\NormalTok{.filename2index(img\_path)}

\NormalTok{test\_folder }\OperatorTok{=} \StringTok{\textquotesingle{}/content/drive/MyDrive/test\_set\textquotesingle{}}
\NormalTok{test\_transform }\OperatorTok{=}\NormalTok{ transforms.Compose([}
\NormalTok{    transforms.Resize((}\DecValTok{64}\NormalTok{, }\DecValTok{64}\NormalTok{)),}
\NormalTok{    transforms.ToTensor(),}
\NormalTok{    transforms.Normalize((}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{), (}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{))}
\NormalTok{])}
\NormalTok{test\_dataset }\OperatorTok{=}\NormalTok{ CustomImageDataset(test\_folder, transform}\OperatorTok{=}\NormalTok{test\_transform)}
\NormalTok{test\_loader }\OperatorTok{=}\NormalTok{ DataLoader(test\_dataset, batch\_size}\OperatorTok{=}\DecValTok{1}\NormalTok{, shuffle}\OperatorTok{=}\VariableTok{False}\NormalTok{)}

\NormalTok{class\_to\_idx }\OperatorTok{=}\NormalTok{ train\_dataset.class\_to\_idx}
\NormalTok{idx\_to\_class }\OperatorTok{=}\NormalTok{ \{v: k }\ControlFlowTok{for}\NormalTok{ k, v }\KeywordTok{in}\NormalTok{ class\_to\_idx.items()\}}

\CommentTok{\# Make predictions}
\KeywordTok{def}\NormalTok{ evaluate\_model(model, test\_loader, idx\_to\_class):}
\NormalTok{    all\_predictions }\OperatorTok{=}\NormalTok{ \{\}}
    \ControlFlowTok{with}\NormalTok{ torch.no\_grad():}
        \ControlFlowTok{for}\NormalTok{ inputs, index }\KeywordTok{in}\NormalTok{ test\_loader:}
\NormalTok{            inputs }\OperatorTok{=}\NormalTok{ inputs.to(device)}

\NormalTok{            outputs }\OperatorTok{=}\NormalTok{ model(inputs)}
\NormalTok{            \_, predicted }\OperatorTok{=}\NormalTok{ torch.}\BuiltInTok{max}\NormalTok{(outputs, }\DecValTok{1}\NormalTok{)}
\NormalTok{            predicted\_class }\OperatorTok{=}\NormalTok{ predicted.item()}
\NormalTok{            predicted\_class\_name }\OperatorTok{=}\NormalTok{ idx\_to\_class[predicted\_class]}
\NormalTok{            all\_predictions[index[}\DecValTok{0}\NormalTok{]] }\OperatorTok{=}\NormalTok{ predicted\_class\_name}

    \ControlFlowTok{return}\NormalTok{ all\_predictions}

\NormalTok{predictions }\OperatorTok{=}\NormalTok{ evaluate\_model(model, test\_loader, idx\_to\_class)}
\ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(}\StringTok{\textquotesingle{}predictions.json\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}w\textquotesingle{}}\NormalTok{) }\ImportTok{as}\NormalTok{ json\_file:}
\NormalTok{    json.dump(predictions, json\_file, indent}\OperatorTok{=}\DecValTok{4}\NormalTok{)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Evaluation completed and predictions saved."}\NormalTok{)}

\CommentTok{\# you may need to install thop when you first run this code}
\OperatorTok{!}\NormalTok{pip install thop}

\CommentTok{\# Compute FLOPs using thop}
\ImportTok{import}\NormalTok{ thop}
\NormalTok{input\_tensor }\OperatorTok{=}\NormalTok{ test\_dataset[}\DecValTok{0}\NormalTok{][}\DecValTok{0}\NormalTok{].unsqueeze(}\DecValTok{0}\NormalTok{).to(device) }\CommentTok{\# must have exact same size of the data input (batch, channel, height, width) and be on the same device as the model}
\NormalTok{flops, params }\OperatorTok{=}\NormalTok{ thop.profile(model, inputs}\OperatorTok{=}\NormalTok{(input\_tensor,))}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"FLOPs: }\SpecialCharTok{\{}\NormalTok{flops}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Number of Parameters: }\SpecialCharTok{\{}\NormalTok{params}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{flops\_and\_params }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"FLOPs"}\NormalTok{: flops,}
    \StringTok{"Parameters"}\NormalTok{: params}
\NormalTok{\}}

\NormalTok{output\_json\_path }\OperatorTok{=} \StringTok{\textquotesingle{}flops\_and\_params.json\textquotesingle{}}

\ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(output\_json\_path, }\StringTok{\textquotesingle{}w\textquotesingle{}}\NormalTok{) }\ImportTok{as}\NormalTok{ json\_file:}
\NormalTok{    json.dump(flops\_and\_params, json\_file, indent}\OperatorTok{=}\DecValTok{4}\NormalTok{)}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"FLOPs and parameters have been saved to }\SpecialCharTok{\{}\NormalTok{output\_json\_path}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Val Accuracy: 69.94%
\end{verbatim}

\phantomsection\label{prob1}
\begin{verbatim}
69.94328922495274
\end{verbatim}

\section{2 Comparing Vanilla RNN with Variants in Sequence Modeling
{[}20
points{]}}\label{comparing-vanilla-rnn-with-variants-in-sequence-modeling-20-points}

You will implement and train three different neural networks for
sequence modeling: a Vanilla RNN (a simple RNN with shared weights), and
two variants of a NN with a similar architecture to the Vanilla RNN but
which do not share weights.

You will compare their performance on a sequence prediction task and
analyze the differences between them.

Prediction task: this is a many-to-one regression task, i.e., a sequence
of inputs is used for predicting a single output. In particular, the
i-th input sequence
\(\mathcal{X}^{(i)}=(x_{1}^{(i)},...,x_{l_{i}}^{(i)})\) has length
\(l_{i}\), where \(x_{j}^{(i)}\in\mathbb{R}^{10}\), the i-th output is a
scalar \(y^{(i)}\in\mathbb{R}\).

Dataset: You will use a synthetic dataset containing sequences of
variable lengths stored in the zip file homework5\_question2\_data.zip.
Each sequence consists of input features and corresponding target
values. The sequences are generated such that they represent a
time-dependent process. Note that \(l_{i}\) may be different than
\(l_{j}\) for \(i\ne j\). So the (pickled) numpy object X is actually a
list of sequences.

Tasks: 1. (4 points) Implement a Vanilla RNN: Implement a Vanilla RNN
architecture (needless to say, weights are shared across time steps). A
pytorch starter code is provided in homework4\_starter.py. Important:
You are not allowed to use an RNN layer implementation from any library.
2. (4 points) Implement a NN with Sequences Truncated to the Same
Length: Implement a NN where sequences are truncated to have the same
length before training. In other words, if the shortest sequence in the
dataset has length L, all sequences should be truncated to length L
before training. 3. (4 points) Implement a NN with Sequences Padded to
the Same Length: Implement another variant of NN where sequences are
padded to have the same length before training. Use appropriate padding
techniques to ensure that all sequences have the same length, and
implement a mechanism to ignore the padding when computing loss and
predictions. 4. Train and Compare the Models: (a) (1 point) Train all
three models (Vanilla RNN, Truncated NN, Padded NN) on the provided
dataset. (b) (1 point) Use a suitable loss function for sequence
prediction tasks, such as mean squared error (MSE) or cross-entropy. (c)
(1 point) Train each model for a fixed number of epochs or until
convergence. (d) (1 point) Monitor and record performance metrics, such
as training loss, on a validation set during training. 5. Evaluate and
Compare the Models: (a) (1 point) Evaluate the trained models on a
separate test dataset. (b) (2 point) Compare the performance of the
three models in terms of MSE, convergence speed, and overfitting
tendencies. (c) (1 point) Analyze the results and discuss the advantages
and disadvantages of each approach in terms of modeling sequences with
varying lengths.

Additional Information: You can choose the specific hyperparameters for
your models, such as the number of hidden units, learning rate, batch
size, and sequence length. Feel free to use any deep learning framework
or library you are comfortable with, and provide clear code
documentation. Note: Be sure to clearly explain your implementation,
provide code comments, and present your results in a well-organized
manner in the report.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Answer}\label{answer-1}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# connect to google drive}
\ImportTok{from}\NormalTok{ google.colab }\ImportTok{import}\NormalTok{ drive}
\NormalTok{drive.mount(}\StringTok{\textquotesingle{}/content/drive/\textquotesingle{}}\NormalTok{)}

\NormalTok{path }\OperatorTok{=} \StringTok{\textquotesingle{}/content/drive/MyDrive/\textquotesingle{}}

\ImportTok{import}\NormalTok{ torch}
\ImportTok{import}\NormalTok{ torch.nn }\ImportTok{as}\NormalTok{ nn}
\ImportTok{import}\NormalTok{ torch.optim }\ImportTok{as}\NormalTok{ optim}
\ImportTok{from}\NormalTok{ torch.nn.utils.rnn }\ImportTok{import}\NormalTok{ pad\_sequence}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ random}
\ImportTok{import}\NormalTok{ os}
\NormalTok{os.chdir(path)}


\CommentTok{\# load training and test data}
\KeywordTok{def}\NormalTok{ loadData():}
\NormalTok{    X\_train }\OperatorTok{=}\NormalTok{ np.load(}\StringTok{\textquotesingle{}X\_train.npy\textquotesingle{}}\NormalTok{,allow\_pickle}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{    y\_train }\OperatorTok{=}\NormalTok{ np.load(}\StringTok{\textquotesingle{}y\_train.npy\textquotesingle{}}\NormalTok{,allow\_pickle}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{    X\_test }\OperatorTok{=}\NormalTok{ np.load(}\StringTok{\textquotesingle{}X\_test.npy\textquotesingle{}}\NormalTok{,allow\_pickle}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{    y\_test }\OperatorTok{=}\NormalTok{ np.load(}\StringTok{\textquotesingle{}y\_test.npy\textquotesingle{}}\NormalTok{,allow\_pickle}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

\NormalTok{    X\_train }\OperatorTok{=}\NormalTok{ [torch.Tensor(x) }\ControlFlowTok{for}\NormalTok{ x }\KeywordTok{in}\NormalTok{ X\_train]  }\CommentTok{\# List of Tensors (SEQ\_LEN[i],INPUT\_DIM) i=0..NUM\_SAMPLES{-}1}
\NormalTok{    X\_test }\OperatorTok{=}\NormalTok{ [torch.Tensor(x) }\ControlFlowTok{for}\NormalTok{ x }\KeywordTok{in}\NormalTok{ X\_test]  }\CommentTok{\# List of Tensors (SEQ\_LEN[i],INPUT\_DIM)}
\NormalTok{    y\_train }\OperatorTok{=}\NormalTok{ torch.Tensor(y\_train) }\CommentTok{\# (NUM\_SAMPLES,1)}
\NormalTok{    y\_test }\OperatorTok{=}\NormalTok{ torch.Tensor(y\_test) }\CommentTok{\# (NUM\_SAMPLES,1)}

    \ControlFlowTok{return}\NormalTok{ X\_train, X\_test, y\_train, y\_test}


\CommentTok{\# Define a Vanilla RNN layer by hand}
\KeywordTok{class}\NormalTok{ RNNLayer(nn.Module):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, input\_size, hidden\_size):}
        \BuiltInTok{super}\NormalTok{(RNNLayer, }\VariableTok{self}\NormalTok{).}\FunctionTok{\_\_init\_\_}\NormalTok{()}
        \VariableTok{self}\NormalTok{.hidden\_size }\OperatorTok{=}\NormalTok{ hidden\_size}
        \VariableTok{self}\NormalTok{.input\_size }\OperatorTok{=}\NormalTok{ input\_size}
        \VariableTok{self}\NormalTok{.W\_xh }\OperatorTok{=}\NormalTok{ nn.Parameter(torch.randn(input\_size, hidden\_size) }\OperatorTok{*} \FloatTok{0.01}\NormalTok{)}
        \VariableTok{self}\NormalTok{.W\_hh }\OperatorTok{=}\NormalTok{ nn.Parameter(torch.randn(hidden\_size, hidden\_size) }\OperatorTok{*} \FloatTok{0.01}\NormalTok{)}
        \VariableTok{self}\NormalTok{.activation }\OperatorTok{=}\NormalTok{ torch.tanh}

    \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, x, hidden):}
\NormalTok{        hidden }\OperatorTok{=} \VariableTok{self}\NormalTok{.activation(x }\OperatorTok{@} \VariableTok{self}\NormalTok{.W\_xh }\OperatorTok{+}\NormalTok{ hidden }\OperatorTok{@} \VariableTok{self}\NormalTok{.W\_hh)}
        \ControlFlowTok{return}\NormalTok{ hidden}

\CommentTok{\# Define a sequence prediction model using the Vanilla RNN}
\KeywordTok{class}\NormalTok{ SequenceModel(nn.Module):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, input\_size, hidden\_size, output\_size):}
        \BuiltInTok{super}\NormalTok{(SequenceModel, }\VariableTok{self}\NormalTok{).}\FunctionTok{\_\_init\_\_}\NormalTok{()}
        \VariableTok{self}\NormalTok{.hidden\_size }\OperatorTok{=}\NormalTok{ hidden\_size}
        \VariableTok{self}\NormalTok{.rnn }\OperatorTok{=}\NormalTok{ RNNLayer(input\_size, hidden\_size)}
        \VariableTok{self}\NormalTok{.linear }\OperatorTok{=}\NormalTok{ nn.Linear(hidden\_size, output\_size)}

    \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, input\_seq, seq\_lengths):}
\NormalTok{        batch\_size }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(input\_seq)}
\NormalTok{        last\_hidden }\OperatorTok{=}\NormalTok{ torch.zeros(batch\_size, }\VariableTok{self}\NormalTok{.hidden\_size, device}\OperatorTok{=}\NormalTok{device)}

        \ControlFlowTok{for}\NormalTok{ b }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(batch\_size):}
\NormalTok{            hidden }\OperatorTok{=}\NormalTok{ torch.zeros(}\VariableTok{self}\NormalTok{.hidden\_size, device}\OperatorTok{=}\NormalTok{device)}

\NormalTok{            seq\_length }\OperatorTok{=}\NormalTok{  seq\_lengths[b]}

            \ControlFlowTok{for}\NormalTok{ t }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(seq\_length):}
\NormalTok{                hidden }\OperatorTok{=} \VariableTok{self}\NormalTok{.rnn(input\_seq[b][t], hidden)}

            \CommentTok{\# Store the last hidden state in the output tensor}
\NormalTok{            last\_hidden[b] }\OperatorTok{=}\NormalTok{ hidden}

\NormalTok{        output }\OperatorTok{=} \VariableTok{self}\NormalTok{.linear(last\_hidden)}
        \ControlFlowTok{return}\NormalTok{ output}

\CommentTok{\# Define a sequence prediction model for fixed length sequences, BUT NO SHARED WEIGHTS}
\KeywordTok{class}\NormalTok{ SequenceModelFixedLen(nn.Module):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, input\_size, hidden\_size, output\_size, seq\_len):}
        \BuiltInTok{super}\NormalTok{(SequenceModelFixedLen, }\VariableTok{self}\NormalTok{).}\FunctionTok{\_\_init\_\_}\NormalTok{()}
        \VariableTok{self}\NormalTok{.hidden\_size }\OperatorTok{=}\NormalTok{ hidden\_size}
        \VariableTok{self}\NormalTok{.seq\_len }\OperatorTok{=}\NormalTok{ seq\_len}
        \VariableTok{self}\NormalTok{.rnn\_layers }\OperatorTok{=}\NormalTok{ nn.ModuleList([RNNLayer(input\_size, hidden\_size) }\ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(seq\_len)])}
        \VariableTok{self}\NormalTok{.linear }\OperatorTok{=}\NormalTok{ nn.Linear(hidden\_size, output\_size)}

    \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, input\_seq, seq\_lengths):}
\NormalTok{        batch\_size }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(input\_seq)}
\NormalTok{        last\_hidden }\OperatorTok{=}\NormalTok{ torch.zeros(batch\_size, }\VariableTok{self}\NormalTok{.hidden\_size, device}\OperatorTok{=}\NormalTok{device)}

        \ControlFlowTok{for}\NormalTok{ b }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(batch\_size):}
\NormalTok{            hidden }\OperatorTok{=}\NormalTok{ torch.zeros(}\VariableTok{self}\NormalTok{.hidden\_size, device}\OperatorTok{=}\NormalTok{device).to(device)}

\NormalTok{            seq\_length }\OperatorTok{=} \BuiltInTok{min}\NormalTok{(}\VariableTok{self}\NormalTok{.seq\_len, seq\_lengths[b]) }
            \ControlFlowTok{for}\NormalTok{ t }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(seq\_length):}
\NormalTok{                hidden }\OperatorTok{=} \VariableTok{self}\NormalTok{.rnn\_layers[t](input\_seq[b][t], hidden)}

            \CommentTok{\# Store the last hidden state in the output tensor}
\NormalTok{            last\_hidden[b] }\OperatorTok{=}\NormalTok{ hidden}

\NormalTok{        output }\OperatorTok{=} \VariableTok{self}\NormalTok{.linear(last\_hidden)}
        \ControlFlowTok{return}\NormalTok{ output}



\KeywordTok{class}\NormalTok{ PaddedModel(nn.Module):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, input\_size, hidden\_size, output\_size, seq\_len\_max):}
        \BuiltInTok{super}\NormalTok{(PaddedModel, }\VariableTok{self}\NormalTok{).}\FunctionTok{\_\_init\_\_}\NormalTok{()}
        \VariableTok{self}\NormalTok{.hidden\_size }\OperatorTok{=}\NormalTok{ hidden\_size}
        \VariableTok{self}\NormalTok{.seq\_len\_max }\OperatorTok{=}\NormalTok{ seq\_len\_max}
        \VariableTok{self}\NormalTok{.rnn\_layers }\OperatorTok{=}\NormalTok{ nn.ModuleList([RNNLayer(input\_size, hidden\_size) }\ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(seq\_len\_max)])}
        \VariableTok{self}\NormalTok{.linear }\OperatorTok{=}\NormalTok{ nn.Linear(hidden\_size, output\_size)}

    \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, padded\_batch, lengths):}
\NormalTok{        B, T, \_ }\OperatorTok{=}\NormalTok{ padded\_batch.shape}
\NormalTok{        device }\OperatorTok{=}\NormalTok{ padded\_batch.device}

\NormalTok{        hidden }\OperatorTok{=}\NormalTok{ [torch.zeros(}\VariableTok{self}\NormalTok{.hidden\_size, device}\OperatorTok{=}\NormalTok{device) }\ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(B)]}

        \ControlFlowTok{for}\NormalTok{ t }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(T):}
            \ControlFlowTok{for}\NormalTok{ b }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(B):}
                \ControlFlowTok{if}\NormalTok{ t }\OperatorTok{\textless{}}\NormalTok{ lengths[b]:}

\NormalTok{                    hidden[b] }\OperatorTok{=} \VariableTok{self}\NormalTok{.rnn\_layers[t](padded\_batch[b, t], hidden[b])}

\NormalTok{        last\_hidden }\OperatorTok{=}\NormalTok{ torch.stack(hidden, dim}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
        \ControlFlowTok{return} \VariableTok{self}\NormalTok{.linear(last\_hidden)}



\CommentTok{\# Define hyperparameters and other settings}
\NormalTok{input\_size }\OperatorTok{=} \DecValTok{10}  \CommentTok{\# Replace with the actual dimension of your input features}
\NormalTok{hidden\_size }\OperatorTok{=} \DecValTok{64}
\NormalTok{output\_size }\OperatorTok{=} \DecValTok{1}
\NormalTok{num\_epochs }\OperatorTok{=} \DecValTok{10}
\NormalTok{learning\_rate }\OperatorTok{=} \FloatTok{0.001}
\NormalTok{batch\_size }\OperatorTok{=} \DecValTok{32}


\CommentTok{\# load data}
\NormalTok{X\_train, X\_test, y\_train, y\_test }\OperatorTok{=}\NormalTok{ loadData()}
\NormalTok{device }\OperatorTok{=}\NormalTok{ y\_train.device}

\CommentTok{\# Create the model using min length input}
\NormalTok{seq\_lengths }\OperatorTok{=}\NormalTok{ [seq.shape[}\DecValTok{0}\NormalTok{] }\ControlFlowTok{for}\NormalTok{ seq }\KeywordTok{in}\NormalTok{ X\_train]}


\NormalTok{all\_indices }\OperatorTok{=}\NormalTok{ np.arange(}\BuiltInTok{len}\NormalTok{(X\_train))}
\NormalTok{np.random.shuffle(all\_indices)}

\NormalTok{train\_cutoff }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(}\FloatTok{0.8} \OperatorTok{*} \BuiltInTok{len}\NormalTok{(all\_indices))}
\NormalTok{train\_indices }\OperatorTok{=}\NormalTok{ all\_indices[:train\_cutoff]}
\NormalTok{val\_indices   }\OperatorTok{=}\NormalTok{ all\_indices[train\_cutoff:]}


\NormalTok{X\_train\_split }\OperatorTok{=}\NormalTok{ []}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in}\NormalTok{ train\_indices:}
\NormalTok{    X\_train\_split.append(X\_train[i])}
\NormalTok{y\_train\_split }\OperatorTok{=}\NormalTok{ y\_train[train\_indices]}


\NormalTok{X\_val\_split }\OperatorTok{=}\NormalTok{ []}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in}\NormalTok{ val\_indices:}
\NormalTok{    X\_val\_split.append(X\_train[i])}
\NormalTok{y\_val\_split }\OperatorTok{=}\NormalTok{ y\_train[val\_indices]}


\CommentTok{\# Training loop}
\KeywordTok{def}\NormalTok{ train(model, num\_epochs, lr, batch\_size, X\_train, y\_train, seq\_lengths):}
\NormalTok{    criterion }\OperatorTok{=}\NormalTok{ nn.MSELoss()}
\NormalTok{    optimizer }\OperatorTok{=}\NormalTok{ optim.Adam(model.parameters(), lr}\OperatorTok{=}\NormalTok{lr)}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"training!"}\NormalTok{)}
    \ControlFlowTok{for}\NormalTok{ epoch }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(num\_epochs):}

        \BuiltInTok{print}\NormalTok{(}\StringTok{"epoch "}\NormalTok{, epoch)}

        \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{0}\NormalTok{, }\BuiltInTok{len}\NormalTok{(X\_train), batch\_size):}
\NormalTok{            inputs }\OperatorTok{=}\NormalTok{ X\_train[i:i}\OperatorTok{+}\NormalTok{batch\_size]}
\NormalTok{            targets }\OperatorTok{=}\NormalTok{ y\_train[i:i}\OperatorTok{+}\NormalTok{batch\_size]}
\NormalTok{            lengths }\OperatorTok{=}\NormalTok{ seq\_lengths[i:i}\OperatorTok{+}\NormalTok{batch\_size]}

            \CommentTok{\#GPU related stuff to ensure it picks the right device}
\NormalTok{            inputs  }\OperatorTok{=}\NormalTok{ [x.to(device) }\ControlFlowTok{for}\NormalTok{ x }\KeywordTok{in}\NormalTok{ inputs]}
\NormalTok{            targets }\OperatorTok{=}\NormalTok{ targets.to(device)}

\NormalTok{            optimizer.zero\_grad()}
\NormalTok{            outputs }\OperatorTok{=}\NormalTok{ model(inputs, lengths)}
\NormalTok{            loss }\OperatorTok{=}\NormalTok{ criterion(outputs, targets)}
\NormalTok{            loss.backward()}
\NormalTok{            optimizer.step()}
\NormalTok{        MSE\_val }\OperatorTok{=}\NormalTok{ mse\_padded(model, X\_val\_split, y\_val\_split)}
        \BuiltInTok{print}\NormalTok{(}\StringTok{"MSE "}\NormalTok{, MSE\_val)}
        \BuiltInTok{print}\NormalTok{(loss)}
    \ControlFlowTok{return}\NormalTok{ model}

\KeywordTok{def}\NormalTok{ train\_padded(model, num\_epochs, lr, batch\_size, X\_train, y\_train):}
\NormalTok{    model.train()}
\NormalTok{    optimizer }\OperatorTok{=}\NormalTok{ optim.Adam(model.parameters(), lr}\OperatorTok{=}\NormalTok{lr)}
\NormalTok{    criterion }\OperatorTok{=}\NormalTok{ nn.MSELoss()}

    \BuiltInTok{print}\NormalTok{(}\StringTok{"training padded!"}\NormalTok{)}

    \ControlFlowTok{for}\NormalTok{ epoch }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(num\_epochs):}
        \BuiltInTok{print}\NormalTok{(}\StringTok{"epoch "}\NormalTok{,epoch)}
        \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{0}\NormalTok{, }\BuiltInTok{len}\NormalTok{(X\_train), batch\_size):}
\NormalTok{            batch }\OperatorTok{=}\NormalTok{ X\_train[i:i}\OperatorTok{+}\NormalTok{batch\_size]}
\NormalTok{            targets }\OperatorTok{=}\NormalTok{ y\_train[i:i}\OperatorTok{+}\NormalTok{batch\_size].to(device)}

\NormalTok{            lengths }\OperatorTok{=}\NormalTok{ [}\BuiltInTok{len}\NormalTok{(s) }\ControlFlowTok{for}\NormalTok{ s }\KeywordTok{in}\NormalTok{ batch]}
\NormalTok{            padded }\OperatorTok{=}\NormalTok{ pad\_sequence(batch, batch\_first}\OperatorTok{=}\VariableTok{True}\NormalTok{).to(device)}
\NormalTok{            optimizer.zero\_grad()}
\NormalTok{            outputs }\OperatorTok{=}\NormalTok{ model(padded, lengths)}
\NormalTok{            loss }\OperatorTok{=}\NormalTok{ criterion(outputs, targets)}
\NormalTok{            loss.backward()}
\NormalTok{            optimizer.step()}
\NormalTok{        MSE\_val }\OperatorTok{=}\NormalTok{ mse\_padded(model, X\_val\_split, y\_val\_split)}
        \BuiltInTok{print}\NormalTok{(}\StringTok{"Padded MSE "}\NormalTok{, MSE\_val)}
        \BuiltInTok{print}\NormalTok{(loss.item())}


\KeywordTok{def}\NormalTok{ mse(model, inputs, y):}
\NormalTok{    model.}\BuiltInTok{eval}\NormalTok{()}
\NormalTok{    crit }\OperatorTok{=}\NormalTok{ nn.MSELoss()}
\NormalTok{    preds }\OperatorTok{=}\NormalTok{ []}
\NormalTok{    bs }\OperatorTok{=} \DecValTok{64}
\NormalTok{    lengths }\OperatorTok{=}\NormalTok{ []}

    \ControlFlowTok{for}\NormalTok{ x }\KeywordTok{in}\NormalTok{ inputs:}
\NormalTok{        lengths.append(}\BuiltInTok{len}\NormalTok{(x))}

    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{0}\NormalTok{, }\BuiltInTok{len}\NormalTok{(inputs), bs):}

\NormalTok{        batch }\OperatorTok{=}\NormalTok{ []}
        \ControlFlowTok{for}\NormalTok{ x }\KeywordTok{in}\NormalTok{ inputs[i:i}\OperatorTok{+}\NormalTok{bs]:}
\NormalTok{            batch.append(x.to(device))}

\NormalTok{        lens  }\OperatorTok{=}\NormalTok{ lengths[i:i}\OperatorTok{+}\NormalTok{bs]}
\NormalTok{        preds.append(model(batch, lens))}

\NormalTok{    preds }\OperatorTok{=}\NormalTok{ torch.cat(preds, dim}\OperatorTok{=}\DecValTok{0}\NormalTok{)}

    \ControlFlowTok{return}\NormalTok{ crit(preds, y.to(device)).item()}



\KeywordTok{def}\NormalTok{ mse\_padded(model, inputs, y):}
\NormalTok{    model.}\BuiltInTok{eval}\NormalTok{()}
\NormalTok{    crit }\OperatorTok{=}\NormalTok{ nn.MSELoss()}
\NormalTok{    preds }\OperatorTok{=}\NormalTok{ []}
\NormalTok{    bs }\OperatorTok{=} \DecValTok{64}

\NormalTok{    lengths }\OperatorTok{=}\NormalTok{ []}
    \ControlFlowTok{for}\NormalTok{ x }\KeywordTok{in}\NormalTok{ inputs:}
\NormalTok{        lengths.append(}\BuiltInTok{len}\NormalTok{(x))}

    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{0}\NormalTok{, }\BuiltInTok{len}\NormalTok{(inputs), bs):}
\NormalTok{        batch }\OperatorTok{=}\NormalTok{ []}
        \ControlFlowTok{for}\NormalTok{ x }\KeywordTok{in}\NormalTok{ inputs[i:i}\OperatorTok{+}\NormalTok{bs]:}
\NormalTok{            batch.append(x.to(device))}

\NormalTok{        lens }\OperatorTok{=}\NormalTok{ lengths[i:i}\OperatorTok{+}\NormalTok{bs]}
\NormalTok{        padded }\OperatorTok{=}\NormalTok{ pad\_sequence(batch, batch\_first}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

\NormalTok{        preds.append(model(padded, lens))}

\NormalTok{    preds }\OperatorTok{=}\NormalTok{ torch.cat(preds, dim}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
    \ControlFlowTok{return}\NormalTok{ crit(preds, y.to(device)).item()}



\CommentTok{\# initialize and train Vanilla RNN}
\ControlFlowTok{if} \VariableTok{\_\_name\_\_} \OperatorTok{==} \StringTok{"\_\_main\_\_"}\NormalTok{:}

\NormalTok{    X\_train, X\_test, y\_train, y\_test }\OperatorTok{=}\NormalTok{ loadData()}

    \ControlFlowTok{if}\NormalTok{ torch.cuda.is\_available():}
\NormalTok{        device }\OperatorTok{=}\NormalTok{ torch.device(}\StringTok{"cuda"}\NormalTok{) }\CommentTok{\# pick my gpu}
        \BuiltInTok{print}\NormalTok{(}\StringTok{"cuda selected!"}\NormalTok{)}
    \ControlFlowTok{else}\NormalTok{:}
\NormalTok{        device }\OperatorTok{=}\NormalTok{ torch.device(}\StringTok{"cpu"}\NormalTok{)}
        \BuiltInTok{print}\NormalTok{(}\StringTok{"cpu selected. no visible gpu"}\NormalTok{)}



\NormalTok{    seq\_lengths\_tr  }\OperatorTok{=}\NormalTok{ [}\BuiltInTok{len}\NormalTok{(x) }\ControlFlowTok{for}\NormalTok{ x }\KeywordTok{in}\NormalTok{ X\_train\_split]}
\NormalTok{    seq\_lengths\_val }\OperatorTok{=}\NormalTok{ [}\BuiltInTok{len}\NormalTok{(x) }\ControlFlowTok{for}\NormalTok{ x }\KeywordTok{in}\NormalTok{ X\_val\_split]}

    \BuiltInTok{print}\NormalTok{(}\StringTok{"Vanilla RNN . . . . ."}\NormalTok{)}
\NormalTok{    vanilla }\OperatorTok{=}\NormalTok{ SequenceModel(input\_size, hidden\_size, output\_size).to(device)}
\NormalTok{    train\_vanilla\_RNN }\OperatorTok{=}\NormalTok{train(vanilla, num\_epochs, learning\_rate, batch\_size, X\_train, y\_train, seq\_lengths)}


    \BuiltInTok{print}\NormalTok{ (}\StringTok{"fixed length truncated model...."}\NormalTok{)}


\NormalTok{    Lmin }\OperatorTok{=} \BuiltInTok{min}\NormalTok{(seq\_lengths)}
\NormalTok{    X\_train\_trunc }\OperatorTok{=}\NormalTok{ []}

    \ControlFlowTok{for}\NormalTok{ x }\KeywordTok{in}\NormalTok{ X\_train:}
\NormalTok{         truncated\_seq }\OperatorTok{=}\NormalTok{ x[:Lmin]}
\NormalTok{         X\_train\_trunc.append(truncated\_seq)}

\NormalTok{    seq\_lengths\_trunc }\OperatorTok{=}\NormalTok{ [Lmin] }\OperatorTok{*} \BuiltInTok{len}\NormalTok{(X\_train\_trunc)}


\NormalTok{    trunc }\OperatorTok{=}\NormalTok{ SequenceModelFixedLen(input\_size, hidden\_size, output\_size, seq\_len}\OperatorTok{=}\NormalTok{Lmin).to(device)}
\NormalTok{    Train\_trunc }\OperatorTok{=}\NormalTok{ train(trunc, num\_epochs, learning\_rate, batch\_size, X\_train\_trunc, y\_train, seq\_lengths\_trunc)}



    \BuiltInTok{print}\NormalTok{(}\StringTok{"padded model ...."}\NormalTok{)}
\NormalTok{    Lmax }\OperatorTok{=} \BuiltInTok{max}\NormalTok{(seq\_lengths)}
\NormalTok{    padded\_model }\OperatorTok{=}\NormalTok{ PaddedModel(input\_size, hidden\_size, output\_size, seq\_len\_max}\OperatorTok{=}\NormalTok{Lmax).to(device)}
\NormalTok{    train\_padded(padded\_model, num\_epochs, learning\_rate, batch\_size, X\_train, y\_train)}


    \BuiltInTok{print}\NormalTok{(}\StringTok{"testing each"}\NormalTok{)}
\NormalTok{    vanilla\_test }\OperatorTok{=}\NormalTok{ mse(vanilla, X\_test, y\_test)}

\NormalTok{    trunc\_test }\OperatorTok{=}\NormalTok{ []}

    \ControlFlowTok{for}\NormalTok{ x }\KeywordTok{in}\NormalTok{ X\_test:}
\NormalTok{        truncated\_seq }\OperatorTok{=}\NormalTok{ x[:Lmin]}
\NormalTok{        trunc\_test.append(truncated\_seq)}

\NormalTok{    test\_trunc   }\OperatorTok{=}\NormalTok{ mse(trunc, trunc\_test, y\_test)}

\NormalTok{    padded\_test  }\OperatorTok{=}\NormalTok{ mse\_padded(padded\_model, X\_test, y\_test)}

    \BuiltInTok{print}\NormalTok{(}\StringTok{"vanilla test!!  "}\NormalTok{, vanilla\_test , }\StringTok{"truncated test!! "}\NormalTok{ , test\_trunc , }\StringTok{"Padded Test!! "}\NormalTok{, padded\_test)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"vanilla test!!  "}\NormalTok{, vanilla\_test , }\StringTok{"truncated test!! "}\NormalTok{ , test\_trunc , }\StringTok{"Padded Test!! "}\NormalTok{, padded\_test)}


\CommentTok{\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#}

\CommentTok{\# initialize and train Sequential NN fixing \#timesteps to the minimum sequence length}

\CommentTok{\# initialize and train Sequential NN fixing \#timesteps to the maximum sequence length}
\CommentTok{\# }\AlertTok{NOTE}\CommentTok{: it is OK to use torch.nn.utils.rnn.pad\_sequence; make sure to set parameter batch\_first correctly}
\end{Highlighting}
\end{Shaded}

\phantomsection\label{prob2}
\begin{verbatim}
vanilla test!!   0.00036904800799675286 truncated test!!  0.009197115898132324 Padded Test!!  0.00724533898755908
\end{verbatim}

\section{3 Fine-tune a DistilBERT model {[}20 points + 2 bonus
points{]}}\label{fine-tune-a-distilbert-model-20-points-2-bonus-points}

In this project, you will first train a classification head using a
pre-trained DistilBERT model on a dataset of social media tweets to
classify tweets as containing medical information or not. You are
provided with a dataset of social media tweets, where each tweet is
labeled as either containing medical information (class 1) or not
containing medical information (class 0).

The preprocessing of the dataset, by tokenizing the tweets and
converting them into a format suitable for DistilBERT, is already
provided in the starter code:
\url{https://colab.research.google.com/drive/17syAcTav5Wtq-n_Rs3P1cQ10szIjLlQS?usp=sharing}

Useful documentation for this question can be found here:
\url{https://huggingface.co/docs/transformers/index}
\url{https://huggingface.co/docs/transformers/training}
\url{https://huggingface.co/docs/transformers/tasks/sequence_classification}

You will need to make the following changes to the existing code:

Q1: (2 points) Add 4+ relevant arguments to the parser. Hint: Check how
args is used within load\_pretrained\_and\_finetune and think about
which other arguments should be added. Note: while argparse is designed
to read arguments from the command line, it is currently adapted to work
with a jupyter notebook by passing arguments to parser.parse\_args as a
list.

Q2: (2 points) Split the code in train, val and eval (test) sets
stratified by classes.

Q3: (2 points) Convert sets to HuggingFace Dataset and tokenize using
function tokenize\_batch.

Q4: (6 points) Implement grid search for at least one hyperparameter by
training a classification head for pre-trained DistilBERT model on the
training set and evaluate its performance on the validation set. You can
use DistilBertForSequenceClassification.from\_pretrained from the
transformers library to load the pre-trained model. Note: DO NOT train
the entire model, only the classification head. You can initially freeze
all parameters except the classification head by setting
requires\_grad=False for all parameters in the base model.

Q5: (6 points) Run the final training on train+val with best
hyperparameters.

Q6: (2 points) Based on the item above, discuss the performance of the
model, any challenges faced during fine-tuning, and potential
improvements that can be made to further improve accuracy.

Q7: (BONUS: 2 points) Apply a principled change to your code in order to
achieve F1-macro \textgreater{} 0.50. Explain what you did and why you
did it.

\subsection{4 NOT PART OF HW4: Tensor Shapes in a Transformer Layer {[}0
points{]}}\label{not-part-of-hw4-tensor-shapes-in-a-transformer-layer-0-points}

The figura above shows the transformer layer. The input size of the
transformer layer is {[}10, 90, 20{]} (where 10 represents batch size,
90 represents sequence length, and 20 represents hidden size). We
consider 5 attention heads in this attention layer. The shape of the
(combined) projection matrices is \(H\times H\), which are used to
project the input data to Q (query), K (key), V (value). Please compute
the size of the each output, including: 1. The shape of Q 2. The shape
of K 3. The shape of V 4. The shape of Q for each head 5. The shape of K
for each head 6. The shape of V for each head 7. The shape of the
attention map (output of softmax) 8. The shape of Dropout-1's output 9.
The shape of Output 10. The shape of Dropout-2's output 11. Total number
of the parameters in this transformer layer

For this homework, you are encouraged to experiment with various models
and training strategies. To achieve a full score, your model must
achieve at least 60\% accuracy on the test set.

\section{Submission}\label{submission}

Submit one PDF file that includes your notes for the theoretical
problems (scanned or typed) and screenshots of your code for the
programming problems. All material in the submitted PDF must be
presented in a clear and readable format.




\end{document}

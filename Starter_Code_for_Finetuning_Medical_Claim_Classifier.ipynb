{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning Medical Claim Classifier Problem: Starter Code\n"
      ],
      "metadata": {
        "id": "tBZS5wnPjWXo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connecting to Google Drive (Optional)\n",
        "\n",
        "We recommend you to save files in your Google Drive to avoid having to download every time you run this notebook."
      ],
      "metadata": {
        "id": "lWYYSboejl02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "\n",
        "# Change the code below to whichever folder you would like your directory to be at. Remove # and run.\n",
        "path = '/content/gdrive/MyDrive/medclaim_detection'\n",
        "%cd {path}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4xu4JB6jkQZ",
        "outputId": "57fd916d-ea1e-4682-cf90-2a9085a11d8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n",
            "/content/gdrive/MyDrive/medclaim_detection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries\n",
        "\n",
        "We will use HuggingFace's `transformers` libraries to load a pre-trained DistilBert model."
      ],
      "metadata": {
        "id": "AjI_w7IIkytc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "import re\n",
        "from typing import Dict\n",
        "import itertools\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from datasets import Dataset\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import (\n",
        "    DistilBertForSequenceClassification,\n",
        "    DistilBertTokenizerFast,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from torchvision.ops import sigmoid_focal_loss"
      ],
      "metadata": {
        "id": "TWiIavn2krk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use logging for storing various pieces of information during training."
      ],
      "metadata": {
        "id": "u96o_OwWlLLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))"
      ],
      "metadata": {
        "id": "bL177EDTkwXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e25e7d8c"
      },
      "source": [
        "def download_data(args):\n",
        "    \"\"\"Checks if data file exists and downloads it if not.\"\"\"\n",
        "    file_path = os.path.join(args.data_dir, 'Twitter.csv')\n",
        "    url = 'https://raw.githubusercontent.com/LCS2-IIITD/LESA-EACL-2021/main/data/Twitter.csv'\n",
        "\n",
        "    # Check if data_dir exists, if not, make folder\n",
        "    if not os.path.exists(args.data_dir):\n",
        "        os.makedirs(args.data_dir)\n",
        "        logger.info('Created directory: %s', args.data_dir)\n",
        "\n",
        "    if not os.path.exists(file_path):\n",
        "        logger.info('Downloading Twitter.csv from %s', url)\n",
        "        try:\n",
        "            r = requests.get(url, allow_redirects=True)\n",
        "            with open(file_path, 'wb') as f:\n",
        "                f.write(r.content)\n",
        "            logger.info('Downloaded Twitter.csv to %s', file_path)\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            logger.error('Error downloading file: %s', e)\n",
        "            raise\n",
        "\n",
        "    args.data_path = file_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Apply light preprocessing and add `clean_text` column.\"\"\"\n",
        "    df = df.copy()\n",
        "    df['hashtag'] = df['tweet_text'].apply(lambda x: re.findall(r'#(\\w+)', str(x)))\n",
        "    df['clean_text'] = df['tweet_text'].apply(lambda x: re.sub(r'http\\S+|www\\S+|@[\\S]+', '', str(x)))\n",
        "\n",
        "    tokenizer = TweetTokenizer()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    def tok_and_clean(text: str):\n",
        "        toks = tokenizer.tokenize(text.lower())\n",
        "        toks = [lemmatizer.lemmatize(t) for t in toks if t.isalpha()]\n",
        "        toks = [t for t in toks if t not in stop_words]\n",
        "        return ' '.join(toks)\n",
        "\n",
        "    df['clean_text'] = df['clean_text'].apply(tok_and_clean)\n",
        "    return df"
      ],
      "metadata": {
        "id": "uNIK1VLklXGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred) -> Dict[str, float]:\n",
        "    logits, labels = pred\n",
        "    # handle logits shape (N,1) or (N,)\n",
        "    logits = np.asarray(logits)\n",
        "    if logits.ndim > 1:\n",
        "        logits = logits.reshape(-1)\n",
        "    probs = 1.0 / (1.0 + np.exp(-logits))\n",
        "    preds = (probs >= 0.5).astype(int)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    precision_arr, recall_arr, f1_arr, _ = precision_recall_fscore_support(labels, preds, labels=[0, 1], zero_division=0)\n",
        "\n",
        "    metrics = {\n",
        "        'accuracy': float(acc),\n",
        "        'precision_class_0': float(precision_arr[0]),\n",
        "        'precision_class_1': float(precision_arr[1]),\n",
        "        'recall_class_0': float(recall_arr[0]),\n",
        "        'recall_class_1': float(recall_arr[1]),\n",
        "        'f1_class_0': float(f1_arr[0]),\n",
        "        'f1_class_1': float(f1_arr[1]),\n",
        "    }\n",
        "    metrics['f1'] = float(np.mean([metrics['f1_class_0'], metrics['f1_class_1']]))\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "W1hSY9W0lamM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pretrained_and_finetune(args):\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('wordnet')\n",
        "\n",
        "    os.makedirs(args.output_dir, exist_ok=True)\n",
        "\n",
        "    logger.info('Loading data from %s', args.data_path)\n",
        "    df = pd.read_csv(args.data_path)\n",
        "    if 'tweet_text' not in df.columns or 'claim' not in df.columns:\n",
        "        raise ValueError(\"Input CSV must contain 'tweet_text' and 'claim' columns\")\n",
        "\n",
        "    # print(df.head(10)) # uncomment to print first 10 rows\n",
        "    df = preprocess(df)\n",
        "    df = df[['clean_text', 'claim']].rename(columns={'claim': 'labels'})\n",
        "\n",
        "    # Q2: split the code in train, val and eval (test) sets stratified by classes\n",
        "    # BEGIN YOUR CODE HERE (~2 lines)\n",
        "    # END YOUR CODE HERE\n",
        "\n",
        "    tokenizer = DistilBertTokenizerFast.from_pretrained(args.model_name)\n",
        "\n",
        "    def tokenize_batch(batch):\n",
        "        return tokenizer(batch['clean_text'], truncation=True, padding='max_length', max_length=args.max_length)\n",
        "\n",
        "    # Q3: Convert sets to HuggingFace Dataset and tokenize using function tokenize_batch\n",
        "    # Use variable names: train_ds, val_ds, eval_ds\n",
        "    # BEGIN YOUR CODE HERE (~6 lines)\n",
        "    # END YOUR CODE HERE\n",
        "\n",
        "    # Kepe only the necessary columns in each dataset\n",
        "    keep_cols = ['input_ids', 'attention_mask', 'labels']\n",
        "    train_ds = train_ds.remove_columns([c for c in train_ds.column_names if c not in keep_cols]).with_format('torch')\n",
        "    val_ds = val_ds.remove_columns([c for c in val_ds.column_names if c not in keep_cols]).with_format('torch')\n",
        "    test_ds = test_ds.remove_columns([c for c in test_ds.column_names if c not in keep_cols]).with_format('torch')\n",
        "\n",
        "    # We'll create models inside the grid loop; keep tokenizer ready\n",
        "\n",
        "    # Device selection: prefer CUDA, then Apple MPS, then CPU\n",
        "    if hasattr(torch, 'cuda') and torch.cuda.is_available():\n",
        "        device = 'cuda'\n",
        "    else:\n",
        "        # MPS (Apple Silicon) support\n",
        "        try:\n",
        "            if getattr(torch, 'has_mps', False) and torch.backends.mps.is_available():\n",
        "                device = 'mps'\n",
        "            else:\n",
        "                device = 'cpu'\n",
        "        except Exception:\n",
        "            device = 'cpu'\n",
        "\n",
        "    # Ensure Trainer knows whether to use CUDA\n",
        "    no_cuda = False if device in ('cuda', 'mps') else True\n",
        "\n",
        "    # Note: model instances are created per-trial inside the grid search loop below.\n",
        "\n",
        "    # Grid search over learning rate (no focal gamma needed)\n",
        "    best_score = -float('inf')\n",
        "    best_params = None\n",
        "\n",
        "    # Q4: Implement grid search for at least one hyperparameter\n",
        "\n",
        "    # Build hyperparameter lists\n",
        "    # BEGIN YOUR CODE HERE (~1-7 lines)\n",
        "    # PRO TIP: you can use itertools.product to create a master list of all combos for grid search\n",
        "\n",
        "    for # COMPLETE THIS LINE\n",
        "        logger.info('WHAT DO YOU WANT TO LOG?')\n",
        "        # instantiate fresh pretrained model for each trial\n",
        "        # NOTE: if num_labels=2, CrossEntropy is assumed by default Trainer;\n",
        "        # if num_labels=1, you will have to implement a CustomTrainer to override MSE as loss\n",
        "        model = # COMPLETE THIS LINE\n",
        "\n",
        "        # Freeze base params and unfreeze classifier layers\n",
        "        for name, p in model.named_parameters():\n",
        "            p.requires_grad = False\n",
        "        for name, p in model.named_parameters():\n",
        "            if # COMPLETE THIS LINE\n",
        "                p.requires_grad = True\n",
        "\n",
        "        # trial output dir\n",
        "        trial_output_dir = os.path.join(args.output_dir, f'CHOOSE_EXP_NAME')\n",
        "        os.makedirs(trial_output_dir, exist_ok=True)\n",
        "\n",
        "        training_args = TrainingArguments(\n",
        "            # INCLUDE RELEVANT ARGUMENTS (~5-10 LINES)\n",
        "            no_cuda=no_cuda,\n",
        "            report_to='none',\n",
        "        )\n",
        "\n",
        "        # initialize the Trainer (~5 lines) or CustomTrainer (~6-8 lines)\n",
        "        trainer = # COMPLETE THIS LINE\n",
        "        )\n",
        "\n",
        "        # move model to device if possible\n",
        "        # BEGIN YOUR CODE HERE (~1-4 lines)\n",
        "        # END YOUR CODE HERE\n",
        "\n",
        "        # train, evaluate, compute score, update best score\n",
        "        # BEGIN YOUR CODE HERE (~6-9 lines)\n",
        "        # END YOUR CODE HERE\n",
        "\n",
        "    # END YOUR CODE HERE\n",
        "\n",
        "    if best_params is None:\n",
        "        raise RuntimeError('Grid search failed to produce any candidate best params')\n",
        "\n",
        "    logger.info('Best hyperparameters from grid search: %s (score=%s)', best_params, best_score)\n",
        "\n",
        "    # Q5: Final training on train+val with best hyperparameters\n",
        "    # instantiate pretrained model and freeze appropriate layers\n",
        "    # BEGIN YOUR CODE HERE (~6 lines)\n",
        "    # END YOUR CODE HERE\n",
        "\n",
        "    # combine train_ds and val_df for final training (i.e., original train_df)\n",
        "    # BEGIN YOUR CODE HERE (~4 lines)\n",
        "    # END YOUR CODE HERE\n",
        "\n",
        "    # set the training arguments\n",
        "    # BEGIN YOUR CODE HERE (~9-14 lines)\n",
        "    # END YOUR CODE HERE\n",
        "\n",
        "    # initialize the Trainer (~5 lines) or CustomTrainer (~6-8 lines)\n",
        "\n",
        "    # move model to device if possible\n",
        "    # BEGIN YOUR CODE HERE (~1-4 lines)\n",
        "    # END YOUR CODE HERE\n",
        "\n",
        "    # train, evaluate, save model\n",
        "    # BEGIN YOUR CODE HERE (~3 lines)\n",
        "    # END YOUR CODE HERE\n",
        "    trainer.train()\n",
        "    trainer.save_model(args.output_dir)"
      ],
      "metadata": {
        "id": "HiJPPeQ_ljdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if __name__ == '__main__':\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--data-dir', type=str, required=True, help='Path to CSV with tweet_text and claim columns')\n",
        "parser.add_argument('--output-dir', type=str, default='finetuned', help='Where to save model and tokenizer')\n",
        "parser.add_argument('--model-name', type=str, default='distilbert-base-uncased', help='Pretrained model name')\n",
        "# Q1. Add relevant arguments\n",
        "# BEGIN YOUR CODE HERE (~5-15 lines)\n",
        "# END YOUR CODE HERE\n",
        "\n",
        "args = parser.parse_args(['--data-dir','data'] # EDIT THIS LINE TO PLAY WITH NON-DEFAULT ARGS\n",
        "print(args)\n",
        "\n",
        "# download file if it doesn't exist yet\n",
        "download_data(args)\n",
        "\n",
        "# load pre-trained and finetune\n",
        "load_pretrained_and_finetune(args)"
      ],
      "metadata": {
        "id": "Xk2T7ZCClm5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q6: Based on the item above, discuss the performance of the model, any challenges faced during fine-tuning, and potential improvements that can be made to further improve accuracy."
      ],
      "metadata": {
        "id": "FckvtBgsJzzA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ENTER YOUR ANSWER HERE"
      ],
      "metadata": {
        "id": "c_mKednfJ3hg"
      }
    }
  ]
}
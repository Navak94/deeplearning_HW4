{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Fq5qQL6nN4f"
      },
      "source": [
        "If you decide to participate in the competition, save the **predictions.json and flops_and_params.json** files in a zip folder with your group name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "euVL16niu7I9"
      },
      "outputs": [],
      "source": [
        "# import packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import json\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9j4LIaCuvJ-0",
        "outputId": "633829dc-a283-408e-c357-38ad8508f5b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# connect to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FkwMavdnMjl"
      },
      "source": [
        "# Step 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRAmLlf5nMjl",
        "outputId": "3066d596-5f31-4050-97c6-e9d1a054694b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "betterCNN(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (fc1): Linear(in_features=16384, out_features=512, bias=True)\n",
            "  (fc2): Linear(in_features=512, out_features=5, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# NEW TOM + NATE BLOCK\n",
        "import torch.nn as nn\n",
        "\n",
        "class betterCNN(nn.Module):\n",
        "    def __init__(self, num_classes=5):\n",
        "        super(betterCNN, self).__init__()\n",
        "\n",
        "        # convolutional block\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "\n",
        "        # convolutional block\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "\n",
        "        # convolutional block\n",
        "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(256)\n",
        "        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn6 = nn.BatchNorm2d(256)\n",
        "\n",
        "        # pooling and dropout\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "        # fully connected layers\n",
        "        self.fc1 = nn.Linear(256 * 8 * 8, 512)\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1 block\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # 2 block\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.bn4(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # 3 block\n",
        "        x = self.conv5(x)\n",
        "        x = self.bn5(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.conv6(x)\n",
        "        x = self.bn6(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # flatten\n",
        "        x = x.view(-1, 256 * 8 * 8)\n",
        "\n",
        "        # fully connected layers\n",
        "        x = self.fc1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "model = betterCNN(num_classes=5)\n",
        "model = model.to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbjSPxfOnMjl"
      },
      "source": [
        "# Step 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "PvU2Nqg5nMjm"
      },
      "outputs": [],
      "source": [
        "# NEW TOM + NATE BLOCK\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# transform with augmentation\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# val transform (no augmentation)\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "trainset_dir = '/content/drive/MyDrive/train_set'\n",
        "\n",
        "full_dataset = datasets.ImageFolder(root=trainset_dir, transform=None)\n",
        "\n",
        "total_size = len(full_dataset)\n",
        "indices = list(range(total_size))\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "train_size = int(0.85 * total_size)\n",
        "val_size = total_size - train_size\n",
        "\n",
        "train_indices = indices[0:train_size]\n",
        "val_indices = indices[train_size:total_size]\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root=trainset_dir, transform=train_transform)\n",
        "train_subset = torch.utils.data.Subset(train_dataset, train_indices)\n",
        "\n",
        "val_dataset = datasets.ImageFolder(root=trainset_dir, transform=val_transform)\n",
        "val_subset = torch.utils.data.Subset(val_dataset, val_indices)\n",
        "\n",
        "train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_subset, batch_size=64, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrC8sgELnMjm"
      },
      "source": [
        "# Step 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "02ctrF30nMjm"
      },
      "outputs": [],
      "source": [
        "# NEW TOM + NATE BLOCK\n",
        "\n",
        "import torch.optim as optim\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1) # added label smoothing\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.002, weight_decay=0.00001) # added weight decay\n",
        "# added learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='max',\n",
        "    factor=0.5,\n",
        "    patience=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7eqAh_AnMjm"
      },
      "source": [
        "# Step 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "tFZZgKDAnMjm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36639347-ba92-4efb-a08e-80fd0697cb43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/60\n",
            " -> Train Loss: 5.3046, Train Acc: 29.71%\n",
            " -> Val Acc: 34.59%\n",
            "----> Best model saved ---> Val Acc: 34.59%\n",
            "\n",
            "Epoch 2/60\n",
            " -> Train Loss: 1.5323, Train Acc: 32.15%\n",
            " -> Val Acc: 35.35%\n",
            "----> Best model saved ---> Val Acc: 35.35%\n",
            "\n",
            "Epoch 3/60\n",
            " -> Train Loss: 1.5163, Train Acc: 33.09%\n",
            " -> Val Acc: 34.22%\n",
            "\n",
            "Epoch 4/60\n",
            " -> Train Loss: 1.5155, Train Acc: 32.62%\n",
            " -> Val Acc: 32.14%\n",
            "\n",
            "Epoch 5/60\n",
            " -> Train Loss: 1.5079, Train Acc: 32.89%\n",
            " -> Val Acc: 33.84%\n",
            "\n",
            "Epoch 6/60\n",
            " -> Train Loss: 1.4937, Train Acc: 34.29%\n",
            " -> Val Acc: 35.35%\n",
            "----> Best model saved ---> Val Acc: 35.35%\n",
            "\n",
            "Epoch 7/60\n",
            " -> Train Loss: 1.4814, Train Acc: 35.49%\n",
            " -> Val Acc: 37.43%\n",
            "----> Best model saved ---> Val Acc: 37.43%\n",
            "\n",
            "Epoch 8/60\n",
            " -> Train Loss: 1.4858, Train Acc: 33.05%\n",
            " -> Val Acc: 41.40%\n",
            "----> Best model saved ---> Val Acc: 41.40%\n",
            "\n",
            "Epoch 9/60\n",
            " -> Train Loss: 1.4622, Train Acc: 35.06%\n",
            " -> Val Acc: 40.83%\n",
            "\n",
            "Epoch 10/60\n",
            " -> Train Loss: 1.4583, Train Acc: 35.16%\n",
            " -> Val Acc: 42.72%\n",
            "----> Best model saved ---> Val Acc: 42.72%\n",
            "\n",
            "Epoch 11/60\n",
            " -> Train Loss: 1.4338, Train Acc: 39.47%\n",
            " -> Val Acc: 47.45%\n",
            "----> Best model saved ---> Val Acc: 47.45%\n",
            "\n",
            "Epoch 12/60\n",
            " -> Train Loss: 1.4223, Train Acc: 39.84%\n",
            " -> Val Acc: 44.99%\n",
            "\n",
            "Epoch 13/60\n",
            " -> Train Loss: 1.4037, Train Acc: 41.78%\n",
            " -> Val Acc: 48.58%\n",
            "----> Best model saved ---> Val Acc: 48.58%\n",
            "\n",
            "Epoch 14/60\n",
            " -> Train Loss: 1.3833, Train Acc: 44.72%\n",
            " -> Val Acc: 52.74%\n",
            "----> Best model saved ---> Val Acc: 52.74%\n",
            "\n",
            "Epoch 15/60\n",
            " -> Train Loss: 1.3749, Train Acc: 45.52%\n",
            " -> Val Acc: 53.88%\n",
            "----> Best model saved ---> Val Acc: 53.88%\n",
            "\n",
            "Epoch 16/60\n",
            " -> Train Loss: 1.3518, Train Acc: 45.55%\n",
            " -> Val Acc: 54.44%\n",
            "----> Best model saved ---> Val Acc: 54.44%\n",
            "\n",
            "Epoch 17/60\n",
            " -> Train Loss: 1.3561, Train Acc: 46.56%\n",
            " -> Val Acc: 55.77%\n",
            "----> Best model saved ---> Val Acc: 55.77%\n",
            "\n",
            "Epoch 18/60\n",
            " -> Train Loss: 1.3447, Train Acc: 47.13%\n",
            " -> Val Acc: 53.50%\n",
            "\n",
            "Epoch 19/60\n",
            " -> Train Loss: 1.3302, Train Acc: 47.93%\n",
            " -> Val Acc: 56.33%\n",
            "----> Best model saved ---> Val Acc: 56.33%\n",
            "\n",
            "Epoch 20/60\n",
            " -> Train Loss: 1.3267, Train Acc: 47.53%\n",
            " -> Val Acc: 53.69%\n",
            "\n",
            "Epoch 21/60\n",
            " -> Train Loss: 1.2972, Train Acc: 49.50%\n",
            " -> Val Acc: 58.60%\n",
            "----> Best model saved ---> Val Acc: 58.60%\n",
            "\n",
            "Epoch 22/60\n",
            " -> Train Loss: 1.3004, Train Acc: 48.63%\n",
            " -> Val Acc: 58.22%\n",
            "\n",
            "Epoch 23/60\n",
            " -> Train Loss: 1.3024, Train Acc: 48.76%\n",
            " -> Val Acc: 59.17%\n",
            "----> Best model saved ---> Val Acc: 59.17%\n",
            "\n",
            "Epoch 24/60\n",
            " -> Train Loss: 1.2810, Train Acc: 49.40%\n",
            " -> Val Acc: 58.79%\n",
            "\n",
            "Epoch 25/60\n",
            " -> Train Loss: 1.2863, Train Acc: 49.93%\n",
            " -> Val Acc: 56.52%\n",
            "\n",
            "Epoch 26/60\n",
            " -> Train Loss: 1.2798, Train Acc: 50.47%\n",
            " -> Val Acc: 58.79%\n",
            "\n",
            "Epoch 27/60\n",
            " -> Train Loss: 1.2417, Train Acc: 52.14%\n",
            " -> Val Acc: 57.28%\n",
            "\n",
            "Epoch 28/60\n",
            " -> Train Loss: 1.2680, Train Acc: 50.70%\n",
            " -> Val Acc: 60.87%\n",
            "----> Best model saved ---> Val Acc: 60.87%\n",
            "\n",
            "Epoch 29/60\n",
            " -> Train Loss: 1.2602, Train Acc: 51.50%\n",
            " -> Val Acc: 60.87%\n",
            "----> Best model saved ---> Val Acc: 60.87%\n",
            "\n",
            "Epoch 30/60\n",
            " -> Train Loss: 1.2411, Train Acc: 51.14%\n",
            " -> Val Acc: 62.19%\n",
            "----> Best model saved ---> Val Acc: 62.19%\n",
            "\n",
            "Epoch 31/60\n",
            " -> Train Loss: 1.2320, Train Acc: 53.38%\n",
            " -> Val Acc: 58.03%\n",
            "\n",
            "Epoch 32/60\n",
            " -> Train Loss: 1.2426, Train Acc: 51.80%\n",
            " -> Val Acc: 59.74%\n",
            "\n",
            "Epoch 33/60\n",
            " -> Train Loss: 1.2218, Train Acc: 52.47%\n",
            " -> Val Acc: 58.98%\n",
            "\n",
            "Epoch 34/60\n",
            " -> Train Loss: 1.2314, Train Acc: 52.61%\n",
            " -> Val Acc: 58.79%\n",
            "\n",
            "Epoch 35/60\n",
            " -> Train Loss: 1.2190, Train Acc: 53.31%\n",
            " -> Val Acc: 62.95%\n",
            "----> Best model saved ---> Val Acc: 62.95%\n",
            "\n",
            "Epoch 36/60\n",
            " -> Train Loss: 1.2169, Train Acc: 53.74%\n",
            " -> Val Acc: 65.03%\n",
            "----> Best model saved ---> Val Acc: 65.03%\n",
            "\n",
            "Epoch 37/60\n",
            " -> Train Loss: 1.2251, Train Acc: 54.75%\n",
            " -> Val Acc: 61.81%\n",
            "\n",
            "Epoch 38/60\n",
            " -> Train Loss: 1.2127, Train Acc: 54.81%\n",
            " -> Val Acc: 63.14%\n",
            "\n",
            "Epoch 39/60\n",
            " -> Train Loss: 1.2040, Train Acc: 55.68%\n",
            " -> Val Acc: 59.74%\n",
            "\n",
            "Epoch 40/60\n",
            " -> Train Loss: 1.2031, Train Acc: 55.68%\n",
            " -> Val Acc: 65.03%\n",
            "----> Best model saved ---> Val Acc: 65.03%\n",
            "\n",
            "Epoch 41/60\n",
            " -> Train Loss: 1.1986, Train Acc: 55.08%\n",
            " -> Val Acc: 64.84%\n",
            "\n",
            "Epoch 42/60\n",
            " -> Train Loss: 1.1956, Train Acc: 55.58%\n",
            " -> Val Acc: 65.41%\n",
            "----> Best model saved ---> Val Acc: 65.41%\n",
            "\n",
            "Epoch 43/60\n",
            " -> Train Loss: 1.1961, Train Acc: 55.61%\n",
            " -> Val Acc: 65.97%\n",
            "----> Best model saved ---> Val Acc: 65.97%\n",
            "\n",
            "Epoch 44/60\n",
            " -> Train Loss: 1.1844, Train Acc: 57.29%\n",
            " -> Val Acc: 68.05%\n",
            "----> Best model saved ---> Val Acc: 68.05%\n",
            "\n",
            "Epoch 45/60\n",
            " -> Train Loss: 1.1734, Train Acc: 57.62%\n",
            " -> Val Acc: 65.97%\n",
            "\n",
            "Epoch 46/60\n",
            " -> Train Loss: 1.1768, Train Acc: 55.75%\n",
            " -> Val Acc: 66.92%\n",
            "\n",
            "Epoch 47/60\n",
            " -> Train Loss: 1.1708, Train Acc: 57.09%\n",
            " -> Val Acc: 63.71%\n",
            "\n",
            "Epoch 48/60\n",
            " -> Train Loss: 1.1605, Train Acc: 59.02%\n",
            " -> Val Acc: 65.60%\n",
            "\n",
            "Epoch 49/60\n",
            " -> Train Loss: 1.1579, Train Acc: 57.82%\n",
            " -> Val Acc: 65.03%\n",
            "\n",
            "Epoch 50/60\n",
            " -> Train Loss: 1.1584, Train Acc: 57.65%\n",
            " -> Val Acc: 65.97%\n",
            "\n",
            "Epoch 51/60\n",
            " -> Train Loss: 1.1315, Train Acc: 59.73%\n",
            " -> Val Acc: 68.24%\n",
            "----> Best model saved ---> Val Acc: 68.24%\n",
            "\n",
            "Epoch 52/60\n",
            " -> Train Loss: 1.1083, Train Acc: 60.49%\n",
            " -> Val Acc: 67.67%\n",
            "\n",
            "Epoch 53/60\n",
            " -> Train Loss: 1.1044, Train Acc: 61.70%\n",
            " -> Val Acc: 67.30%\n",
            "\n",
            "Epoch 54/60\n",
            " -> Train Loss: 1.0942, Train Acc: 61.50%\n",
            " -> Val Acc: 68.24%\n",
            "----> Best model saved ---> Val Acc: 68.24%\n",
            "\n",
            "Epoch 55/60\n",
            " -> Train Loss: 1.0974, Train Acc: 61.53%\n",
            " -> Val Acc: 65.41%\n",
            "\n",
            "Epoch 56/60\n",
            " -> Train Loss: 1.1052, Train Acc: 61.60%\n",
            " -> Val Acc: 68.81%\n",
            "----> Best model saved ---> Val Acc: 68.81%\n",
            "\n",
            "Epoch 57/60\n",
            " -> Train Loss: 1.0926, Train Acc: 61.86%\n",
            " -> Val Acc: 67.30%\n",
            "\n",
            "Epoch 58/60\n",
            " -> Train Loss: 1.0728, Train Acc: 63.30%\n",
            " -> Val Acc: 69.38%\n",
            "----> Best model saved ---> Val Acc: 69.38%\n",
            "\n",
            "Epoch 59/60\n",
            " -> Train Loss: 1.0795, Train Acc: 62.43%\n",
            " -> Val Acc: 66.54%\n",
            "\n",
            "Epoch 60/60\n",
            " -> Train Loss: 1.0918, Train Acc: 61.93%\n",
            " -> Val Acc: 68.62%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# NEW TOM + NATE BLOCK\n",
        "\n",
        "def train(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=30):\n",
        "    best_val_acc = 0.0\n",
        "    patience_counter = 0\n",
        "    early_stop_patience = 10\n",
        "\n",
        "    train_losses = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # train\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss = running_loss + loss.item()\n",
        "\n",
        "            predictions = torch.max(outputs, 1)[1]\n",
        "            train_total = train_total + labels.size(0)\n",
        "            train_correct = train_correct + (predictions == labels).sum().item()\n",
        "\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "        train_acc = 100.0 * train_correct / train_total\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # val\n",
        "        model.eval()\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                predictions = torch.max(outputs, 1)[1]\n",
        "                val_total = val_total + labels.size(0)\n",
        "                val_correct = val_correct + (predictions == labels).sum().item()\n",
        "\n",
        "        val_acc = 100.0 * val_correct / val_total\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "        print(f\" -> Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "        print(f\" -> Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        # new scheduler step\n",
        "        scheduler.step(val_acc)\n",
        "\n",
        "        # save state added\n",
        "        if val_acc >= best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "            print(f\"----> Best model saved ---> Val Acc: {val_acc:.2f}%\")\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter = patience_counter + 1\n",
        "\n",
        "        # early stopping\n",
        "        if patience_counter >= early_stop_patience:\n",
        "            break\n",
        "\n",
        "        print()\n",
        "\n",
        "    return train_losses, val_accuracies\n",
        "\n",
        "train_losses, val_accuracies = train(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    num_epochs=60\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_wEZfF1nMjn"
      },
      "source": [
        "# Step 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "DFYvvCAhnMjn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b4b8d23-ad9e-445d-d89c-e551a195e5be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Accuracy: 69.38%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "69.37618147448015"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "# NEW TOM + NATE BLOCK\n",
        "\n",
        "def evaluate(model, val_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            predictions = torch.max(outputs, 1)[1]\n",
        "            total = total + labels.size(0)\n",
        "            correct = correct + (predictions == labels).sum().item()\n",
        "\n",
        "    accuracy = 100.0 * correct / total\n",
        "    print(f'Val Accuracy: {accuracy:.2f}%')\n",
        "    return accuracy\n",
        "\n",
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "evaluate(model, val_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wdi-7Yopl4dp"
      },
      "source": [
        "Please don't make any change after this line. The only parameters you may modify are those within the \"test_transform\" function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "jdVw9_b0I1ab"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "class CustomImageDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, folder_path, transform=None):\n",
        "        self.folder_path = folder_path\n",
        "        self.transform = transform\n",
        "        self.image_paths = [os.path.join(folder_path, filename) for filename in os.listdir(folder_path) if filename.endswith(('png', 'jpg', 'jpeg'))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def filename2index(self, filename):\n",
        "        return os.path.basename(filename).replace('.jpg', '')\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, self.filename2index(img_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "vxPmHc1OMa_8"
      },
      "outputs": [],
      "source": [
        "test_folder = '/content/drive/MyDrive/test_set'\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "test_dataset = CustomImageDataset(test_folder, transform=test_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "class_to_idx = train_dataset.class_to_idx\n",
        "idx_to_class = {v: k for k, v in class_to_idx.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "2jCDIWpGM0y-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eacf1f17-399d-4df2-eb7b-cbbbe64bac86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation completed and predictions saved.\n"
          ]
        }
      ],
      "source": [
        "# Make predictions\n",
        "def evaluate_model(model, test_loader, idx_to_class):\n",
        "    all_predictions = {}\n",
        "    with torch.no_grad():\n",
        "        for inputs, index in test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            predicted_class = predicted.item()\n",
        "            predicted_class_name = idx_to_class[predicted_class]\n",
        "            all_predictions[index[0]] = predicted_class_name\n",
        "\n",
        "    return all_predictions\n",
        "\n",
        "predictions = evaluate_model(model, test_loader, idx_to_class)\n",
        "with open('predictions.json', 'w') as json_file:\n",
        "    json.dump(predictions, json_file, indent=4)\n",
        "\n",
        "print(\"Evaluation completed and predictions saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "9ym3lmK2Eh36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d56aef0b-ff61-47c0-dbde-67d9e5855efc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: thop in /usr/local/lib/python3.12/dist-packages (0.1.1.post2209072238)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from thop) (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->thop) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->thop) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->thop) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->thop) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->thop) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->thop) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "# you may need to install thop when you first run this code\n",
        "!pip install thop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "BecI85f3EYiA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a75d00ff-d071-4447-bf39-18327e8b0cde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
            "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "FLOPs: 623118848.0\n",
            "Number of Parameters: 9538885.0\n",
            "FLOPs and parameters have been saved to flops_and_params.json\n"
          ]
        }
      ],
      "source": [
        "# Compute FLOPs using thop\n",
        "import thop\n",
        "input_tensor = test_dataset[0][0].unsqueeze(0).to(device) # must have exact same size of the data input (batch, channel, height, width) and be on the same device as the model\n",
        "flops, params = thop.profile(model, inputs=(input_tensor,))\n",
        "print(f\"FLOPs: {flops}\")\n",
        "print(f\"Number of Parameters: {params}\")\n",
        "flops_and_params = {\n",
        "    \"FLOPs\": flops,\n",
        "    \"Parameters\": params\n",
        "}\n",
        "\n",
        "output_json_path = 'flops_and_params.json'\n",
        "\n",
        "with open(output_json_path, 'w') as json_file:\n",
        "    json.dump(flops_and_params, json_file, indent=4)\n",
        "\n",
        "print(f\"FLOPs and parameters have been saved to {output_json_path}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
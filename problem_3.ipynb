{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8962a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "\n",
    "path = '/content/drive/MyDrive/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575d809d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "from typing import Dict\n",
    "import itertools\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import Dataset\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    DistilBertForSequenceClassification,\n",
    "    DistilBertTokenizerFast,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from torchvision.ops import sigmoid_focal_loss\n",
    "\n",
    "\n",
    "import sys\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3d1244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(args):\n",
    "    \"\"\"Checks if data file exists and downloads it if not.\"\"\"\n",
    "    file_path = os.path.join(args.data_dir, 'Twitter.csv')\n",
    "    url = 'https://raw.githubusercontent.com/LCS2-IIITD/LESA-EACL-2021/main/data/Twitter.csv'\n",
    "\n",
    "    # Check if data_dir exists, if not, make folder\n",
    "    if not os.path.exists(args.data_dir):\n",
    "        os.makedirs(args.data_dir)\n",
    "        logger.info('Created directory: %s', args.data_dir)\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        logger.info('Downloading Twitter.csv from %s', url)\n",
    "        try:\n",
    "            r = requests.get(url, allow_redirects=True)\n",
    "            with open(file_path, 'wb') as f:\n",
    "                f.write(r.content)\n",
    "            logger.info('Downloaded Twitter.csv to %s', file_path)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error('Error downloading file: %s', e)\n",
    "            raise\n",
    "\n",
    "    args.data_path = file_path\n",
    "\n",
    "def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Apply light preprocessing and add `clean_text` column.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['hashtag'] = df['tweet_text'].apply(lambda x: re.findall(r'#(\\w+)', str(x)))\n",
    "    df['clean_text'] = df['tweet_text'].apply(lambda x: re.sub(r'http\\S+|www\\S+|@[\\S]+', '', str(x)))\n",
    "\n",
    "    tokenizer = TweetTokenizer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def tok_and_clean(text: str):\n",
    "        toks = tokenizer.tokenize(text.lower())\n",
    "        toks = [lemmatizer.lemmatize(t) for t in toks if t.isalpha()]\n",
    "        toks = [t for t in toks if t not in stop_words]\n",
    "        return ' '.join(toks)\n",
    "\n",
    "    df['clean_text'] = df['clean_text'].apply(tok_and_clean)\n",
    "    return df\n",
    "\n",
    "def compute_metrics(pred) -> Dict[str, float]:\n",
    "    logits, labels = pred\n",
    "    # handle logits shape (N,1) or (N,)\n",
    "    logits = np.asarray(logits)\n",
    "    if logits.ndim > 1:\n",
    "        logits = logits.reshape(-1)\n",
    "    probs = 1.0 / (1.0 + np.exp(-logits))\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision_arr, recall_arr, f1_arr, _ = precision_recall_fscore_support(labels, preds, labels=[0, 1], zero_division=0)\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': float(acc),\n",
    "        'precision_class_0': float(precision_arr[0]),\n",
    "        'precision_class_1': float(precision_arr[1]),\n",
    "        'recall_class_0': float(recall_arr[0]),\n",
    "        'recall_class_1': float(recall_arr[1]),\n",
    "        'f1_class_0': float(f1_arr[0]),\n",
    "        'f1_class_1': float(f1_arr[1]),\n",
    "    }\n",
    "    metrics['f1'] = float(np.mean([metrics['f1_class_0'], metrics['f1_class_1']]))\n",
    "    return metrics\n",
    "\n",
    "\n",
    "#just here for troubleshooting \n",
    "def compute_metrics_debug(eval_pred) -> Dict[str, float]:\n",
    "    \n",
    "    if isinstance(eval_pred, tuple):\n",
    "        predictions, labels = eval_pred\n",
    "    else:\n",
    "        predictions, labels = eval_pred.predictions, eval_pred.label_ids\n",
    "\n",
    " \n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "\n",
    "    predictions = np.asarray(predictions)\n",
    "    labels = np.asarray(labels).reshape(-1)\n",
    "\n",
    "    if predictions.ndim == 2 and predictions.shape[-1] > 1:\n",
    "        preds = np.argmax(predictions, axis=-1)\n",
    "    else:\n",
    "        logits_1d = predictions.reshape(-1)\n",
    "        probs = 1.0 / (1.0 + np.exp(-logits_1d))\n",
    "        preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "\n",
    "    assert preds.shape[0] == labels.shape[0], f\"preds {preds.shape} vs labels {labels.shape}\"\n",
    "\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, labels=[0, 1], average='macro', zero_division=0\n",
    "    )\n",
    "    return {\n",
    "        'accuracy': float(acc),\n",
    "        'precision': float(p),\n",
    "        'recall': float(r),\n",
    "        'f1': float(f1),\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86aa65de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_and_finetune(args):\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "    logger.info('Loading data from %s', args.data_path)\n",
    "    df = pd.read_csv(args.data_path)\n",
    "    if 'tweet_text' not in df.columns or 'claim' not in df.columns:\n",
    "        raise ValueError(\"Input CSV must contain 'tweet_text' and 'claim' columns\")\n",
    "\n",
    "    # print(df.head(10)) # uncomment to print first 10 rows\n",
    "    df = preprocess(df)\n",
    "    df = df[['clean_text', 'claim']].rename(columns={'claim': 'labels'})\n",
    "\n",
    "    # Q2: split the code in train, val and eval (test) sets stratified by classes\n",
    "    # BEGIN YOUR CODE HERE (~2 lines)\n",
    "    ###########################################################################################################3revise\n",
    "    train_val_df, test_df = train_test_split(\n",
    "    df, test_size=args.test_size, stratify=df['labels'], random_state=args.seed)\n",
    "    train_df, val_df = train_test_split(train_val_df,test_size=args.val_size / (1.0 - args.test_size),stratify=train_val_df['labels'],random_state=args.seed)\n",
    "    #############################################################################################################333\n",
    "    # END YOUR CODE HERE\n",
    "\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained(args.model_name)\n",
    "\n",
    "    def tokenize_batch(batch):\n",
    "        return tokenizer(batch['clean_text'], truncation=True, padding='max_length', max_length=args.max_length)\n",
    "\n",
    "    # Q3: Convert sets to HuggingFace Dataset and tokenize using function tokenize_batch\n",
    "    # Use variable names: train_ds, val_ds, eval_ds\n",
    "    # BEGIN YOUR CODE HERE (~6 lines)\n",
    "#####################################################################################################################################################3    \n",
    "    train_ds = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "    val_ds   = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
    "    eval_ds  = Dataset.from_pandas(test_df.reset_index(drop=True))  # \"eval_ds\" = test split\n",
    "\n",
    "    train_ds = train_ds.map(tokenize_batch, batched=True)\n",
    "    val_ds   = val_ds.map(tokenize_batch, batched=True)\n",
    "    eval_ds  = eval_ds.map(tokenize_batch, batched=True)\n",
    "\n",
    "    # keep only the tensor columns the Trainer needs\n",
    "    keep_cols = ['input_ids', 'attention_mask', 'labels']\n",
    "    train_ds = train_ds.remove_columns([c for c in train_ds.column_names if c not in keep_cols]).with_format('torch')\n",
    "    val_ds   = val_ds.remove_columns([c for c in val_ds.column_names if c not in keep_cols]).with_format('torch')\n",
    "    eval_ds  = eval_ds.remove_columns([c for c in eval_ds.column_names if c not in keep_cols]).with_format('torch')\n",
    "####################################################################################################################################################\n",
    "\n",
    "\n",
    "    # END YOUR CODE HERE\n",
    "\n",
    "    # Kepe only the necessary columns in each dataset\n",
    "    #keep_cols = ['input_ids', 'attention_mask', 'labels']\n",
    "    #train_ds = train_ds.remove_columns([c for c in train_ds.column_names if c not in keep_cols]).with_format('torch')\n",
    "    #val_ds = val_ds.remove_columns([c for c in val_ds.column_names if c not in keep_cols]).with_format('torch')\n",
    "    #test_ds = test_ds.remove_columns([c for c in test_ds.column_names if c not in keep_cols]).with_format('torch')\n",
    "\n",
    "    # We'll create models inside the grid loop; keep tokenizer ready\n",
    "\n",
    "    # Device selection: prefer CUDA, then Apple MPS, then CPU\n",
    "    if hasattr(torch, 'cuda') and torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "    else:\n",
    "        try:\n",
    "            if getattr(torch, 'has_mps', False) and torch.backends.mps.is_available():\n",
    "                device = 'mps'\n",
    "            else:\n",
    "                device = 'cpu'\n",
    "        except Exception:\n",
    "            device = 'cpu'\n",
    "\n",
    "    no_cuda = False if device in ('cuda', 'mps') else True\n",
    "\n",
    "    # Note: model instances are created per-trial inside the grid search loop below.\n",
    "\n",
    "    # Grid search over learning rate (no focal gamma needed)\n",
    "    best_score = -float('inf')\n",
    "    best_params = None\n",
    "\n",
    "    # Q4: Implement grid search for at least one hyperparameter\n",
    "##################################################################################################################################3needs a LOT of revision omfg\n",
    "    # Build hyperparameter lists\n",
    "    best_params = None\n",
    "    best_score = -float('inf')\n",
    "\n",
    "    if args.grid_search:\n",
    "        lrs = [0.001, 0.0001, 0.005, 0.00005]\n",
    "        bss = [8, 16,32]\n",
    "        wds = [0.0, 0.01,0.03]\n",
    "        nes = [1,2, 3]\n",
    "        search_space = itertools.product(lrs, bss, wds, nes)\n",
    "    else:\n",
    "        search_space = [(args.learning_rate, args.batch_size, args.weight_decay, args.num_train_epochs)]\n",
    "\n",
    "    # BEGIN YOUR CODE HERE (~1-7 lines)\n",
    "\n",
    "    #######Tom this may trip up on you idk\n",
    "    #make sure the GPU is selected, otherwise default to CPU  \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "    # PRO TIP: you can use itertools.product to create a master list of all combos for grid search\n",
    "\n",
    "    for lr, bs, wd, ne in search_space:# COMPLETE THIS LINE\n",
    "        logger.info(f\"Trial: lr={lr}, bs={bs}, wd={wd}, epochs={ne}\")\n",
    "\n",
    "        # instantiate fresh pretrained model for each trial\n",
    "        # NOTE: if num_labels=2, CrossEntropy is assumed by default Trainer;\n",
    "        # if num_labels=1, you will have to implement a CustomTrainer to override MSE as loss\n",
    "        model = DistilBertForSequenceClassification.from_pretrained(\n",
    "        args.model_name, num_labels=2\n",
    "    )\n",
    "\n",
    "        # Freeze base params and unfreeze classifier layers\n",
    "        for name, p in model.named_parameters():\n",
    "            p.requires_grad = False\n",
    "        for name, p in model.named_parameters():\n",
    "            if name.startswith('classifier') or 'pre_classifier' in name:\n",
    "                p.requires_grad = True\n",
    "\n",
    "        # trial output dir\n",
    "        trial_output_dir = os.path.join(args.output_dir, f'CHOOSE_EXP_NAME')\n",
    "        os.makedirs(trial_output_dir, exist_ok=True)\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=trial_output_dir,\n",
    "            per_device_train_batch_size=bs,\n",
    "            per_device_eval_batch_size=bs,\n",
    "            learning_rate=lr,\n",
    "            num_train_epochs=ne,\n",
    "            weight_decay=wd,\n",
    "            logging_dir=os.path.join(trial_output_dir, \"logs\"),  # safe on old & new\n",
    "        )\n",
    "\n",
    "        # initialize the Trainer (~5 lines) or CustomTrainer (~6-8 lines)\n",
    "        trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        compute_metrics=compute_metrics_debug\n",
    "     # COMPLETE THIS LINE\n",
    "        )\n",
    "        model.to(device)\n",
    "        # move model to device if possible\n",
    "        # BEGIN YOUR CODE HERE (~1-4 lines)\n",
    "        trainer.train()\n",
    "        metrics = trainer.evaluate(eval_dataset=val_ds)\n",
    "        score = metrics.get('eval_f1', -float('inf'))\n",
    "\n",
    "\n",
    "        # END YOUR CODE HERE\n",
    "\n",
    "        # train, evaluate, compute score, update best score\n",
    "        # BEGIN YOUR CODE HERE (~6-9 lines)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params = {'lr': lr, 'bs': bs, 'wd': wd, 'epochs': ne}\n",
    "            print()\n",
    "        # END YOUR CODE HERE\n",
    "\n",
    "    # END YOUR CODE HERE\n",
    "\n",
    "    if best_params is None:\n",
    "        raise RuntimeError('Grid search failed to produce any candidate best params')\n",
    "\n",
    "    logger.info(best_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037d4df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Q5: Final training on train+val with best hyperparameters\n",
    "    # instantiate pretrained model and freeze appropriate layers\n",
    "    # BEGIN YOUR CODE HERE (~6 lines)\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(args.model_name, num_labels=2)\n",
    "\n",
    "    for name, p in model.named_parameters():\n",
    "        p.requires_grad = False\n",
    "    for name, p in model.named_parameters():\n",
    "        if name.startswith('classifier') or 'pre_classifier' in name:\n",
    "            p.requires_grad = True\n",
    "\n",
    "    # END YOUR CODE HE RE\n",
    "\n",
    "    # combine train_ds and val_df for final training (i.e., original train_df)\n",
    "    # BEGIN YOUR CODE HERE (~4 lines)\n",
    "    from datasets import concatenate_datasets\n",
    "    trainval_ds = concatenate_datasets([train_ds, val_ds]).with_format('torch')\n",
    "    final_out_dir = os.path.join(args.output_dir, 'final')\n",
    "    os.makedirs(final_out_dir, exist_ok=True)\n",
    "    # END YOUR CODE HERE\n",
    "\n",
    "    # set the training arguments\n",
    "    # BEGIN YOUR CODE HERE (~9-14 lines)\n",
    "    final_args = TrainingArguments(\n",
    "        output_dir=final_out_dir,\n",
    "        per_device_train_batch_size=best_params['bs'],\n",
    "        per_device_eval_batch_size=best_params['bs'],\n",
    "        learning_rate=best_params['lr'],\n",
    "        num_train_epochs=best_params['epochs'],\n",
    "        weight_decay=best_params['wd'],\n",
    "        logging_dir=os.path.join(final_out_dir, \"logs\"),\n",
    "    )\n",
    "    # END YOUR CODE HERE\n",
    "\n",
    "    # initialize the Trainer (~5 lines) or CustomTrainer (~6-8 lines)\n",
    "    final_trainer = Trainer(\n",
    "        model=model,\n",
    "        args=final_args,\n",
    "        train_dataset=trainval_ds,\n",
    "        eval_dataset=eval_ds,          # held-out test split\n",
    "        compute_metrics=compute_metrics_debug\n",
    "    )\n",
    "\n",
    "    final_trainer.train()\n",
    "    test_metrics = final_trainer.evaluate(eval_dataset=eval_ds)\n",
    "    logger.info(f'Final TEST metrics: {test_metrics}')\n",
    "    final_trainer.save_model(args.output_dir)\n",
    "    # move model to device if possible\n",
    "    # BEGIN YOUR CODE HERE (~1-4 lines)\n",
    "    # END YOUR CODE HERE\n",
    " \n",
    "    # train, evaluate, save model\n",
    "    # BEGIN YOUR CODE HERE (~3 lines)\n",
    "    # END YOUR CODE HERE\n",
    "    trainer.train()\n",
    "    trainer.save_model(args.output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c95d894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data-dir', type=str, required=True, help='Path to CSV with tweet_text and claim columns')\n",
    "parser.add_argument('--output-dir', type=str, default='finetuned', help='Where to save model and tokenizer')\n",
    "parser.add_argument('--model-name', type=str, default='distilbert-base-uncased', help='Pretrained model name')\n",
    "# Q1. Add relevant arguments\n",
    "\n",
    "\n",
    "parser.add_argument('--max-length', type=int, default=128, help='Max token length')\n",
    "parser.add_argument('--batch-size', type=int, default=16, help='Per-device batch size')\n",
    "parser.add_argument('--learning-rate', type=float, default=2e-5, help='LR (used if not grid-searching)')\n",
    "parser.add_argument('--num-train-epochs', type=int, default=3, help='Epochs (used if not grid-searching)')\n",
    "parser.add_argument('--weight-decay', type=float, default=0.01, help='AdamW weight decay')\n",
    "parser.add_argument('--val-size', type=float, default=0.15, help='Validation fraction')\n",
    "parser.add_argument('--test-size', type=float, default=0.15, help='Test fraction')\n",
    "parser.add_argument('--seed', type=int, default=42, help='Random seed')\n",
    "parser.add_argument('--eval-steps', type=int, default=100, help='Evaluate every N steps')\n",
    "parser.add_argument('--logging-steps', type=int, default=50, help='Log every N steps')\n",
    "parser.add_argument('--grid-search', action='store_true', help='Enable grid search over hyperparams')\n",
    "\n",
    "# BEGIN YOUR CODE HERE (~5-15 lines)\n",
    "# END YOUR CODE HERE\n",
    "\n",
    "args = parser.parse_args() # EDIT THIS LINE TO PLAY WITH NON-DEFAULT ARGS\n",
    "print(args)\n",
    "\n",
    "# download file if it doesn't exist yet\n",
    "download_data(args)\n",
    "\n",
    "# load pre-trained and finetune\n",
    "load_pretrained_and_finetune(args)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
